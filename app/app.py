# -------------------------------------------------------------
# ğŸ— AIoT ìŠ¤ë§ˆíŠ¸ ì¸í”„ë¼ ëŒ€ì‹œë³´ë“œ (ì¬ê±´ì¶• ì˜ì‚¬ê²°ì • Helper)
# -------------------------------------------------------------
# ğŸ“Š CSV ê¸°ë°˜ ë°ì´í„° ë°˜ì˜ ë²„ì „ â€” ì •ë¦¬/ê°œì„ ë³¸
# -------------------------------------------------------------

# --- must come first: add project root to sys.path BEFORE importing utils ---
import sys
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent  # .../seoul_dashboard
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))
# ---------------------------------------------------------------------------

import streamlit as st
import pandas as pd
import numpy as np
import pydeck as pdk
import altair as alt
import geopandas as gpd
from datetime import datetime

from components.sidebar_presets import render_sidebar_presets
from components.sidebar_4quadrant_guide import render_sidebar_4quadrant_guide

# === [SESSION KEYS INIT] ===
if "matched_links_geojson" not in st.session_state:
    st.session_state["matched_links_geojson"] = None
if "matched_links_geojson_daily" not in st.session_state:
    st.session_state["matched_links_geojson_daily"] = None

# === ì™¸ë¶€ ëª¨ë“ˆ (utils) ì„í¬íŠ¸ ===
from utils.traffic_preproc import ensure_speed_csv

# Altair/MPL/Plotly ìŠ¤ìœ„ì¹˜í˜•: plot_speedê°€ ì—†ê±°ë‚˜ ëª¨ë“ˆ ìì²´ê°€ ì—†ìœ¼ë©´ ì•ˆì „ í´ë°±
_HAS_PLOT_SPEED = None
_plot_speed = None
_plot_nearby = None
try:
    # ëª¨ë“ˆì´ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸
    import utils.traffic_plot as _tpl
    try:
        from utils.traffic_plot import plot_speed as _plot_speed  # altair ìš°ì„ 
        _HAS_PLOT_SPEED = True
    except Exception as e:
        try:
            from utils.traffic_plot import plot_nearby_speed_from_csv as _plot_nearby
            _HAS_PLOT_SPEED = False
        except Exception:
            _HAS_PLOT_SPEED = None
except Exception:
    _HAS_PLOT_SPEED = None

# === ë°ì´í„° ë””ë ‰í„°ë¦¬ ===
DATA_DIR = BASE_DIR / "data"

# === ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ì´ë¦„ ì¶©ëŒ ë°©ì§€: ë³€ìˆ˜ëª… êµ¬ë¶„) ===
BASE_YEAR = 2023   # 24ë…„ìœ¼ë¡œ ë°”ê¿”ë„ ë¨

# ì¬ê±´ì¶• í”„ë¡œì íŠ¸ ì›ë³¸ CSV (ë‹¹ì‹ ì˜ appì—ì„œ ì“°ë˜ í‘œ ë°ì´í„°)
PROJECTS_CSV_PATH = DATA_DIR / "seoul_redev_projects.csv"

# êµí†µ ê¸°ì¤€ë…„ë„ ë°ì´í„° (ì—‘ì…€ â†’ CSV ìë™ ë³€í™˜ ëŒ€ìƒ)
TRAFFIC_XLSX_PATH = DATA_DIR / "AverageSpeed(LINK).xlsx"
TRAFFIC_CSV_PATH  = DATA_DIR / f"AverageSpeed_Seoul_{BASE_YEAR}.csv"

# ë„ë¡œë§ ë ˆë²¨55 ì‰ì´í”„
SHP_PATH = DATA_DIR / "seoul_link_lev5.5_2023.shp"
LINK_ID_COL = "k_link_id"  # âœ… SHPì˜ ë§í¬ ì»¬ëŸ¼ëª…

#===========================ìºì‹œ ë¹„ìš°ê¸°=================
with st.sidebar:
    if st.button("ìºì‹œ ë¹„ìš°ê¸°"):
        st.cache_data.clear()
        st.session_state["matched_links_geojson"] = None
        st.session_state["matched_links_geojson_daily"] = None
        st.rerun()
#======================================================

# ğŸ”¤ ì•ˆì „í•œ CSV ë¡œë”: ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
def smart_read_csv(path, encodings=("utf-8-sig", "cp949", "euc-kr", "utf-8", "latin1")):
    last_err = None
    for enc in encodings:
        try:
            df = pd.read_csv(path, encoding=enc)
            return df
        except UnicodeDecodeError as e:
            last_err = e
            continue
    # ìµœí›„ ìˆ˜ë‹¨: errors='replace' ë¡œë¼ë„ ì½ê¸°
    try:
        df = pd.read_csv(path, encoding="utf-8", errors="replace")
        return df
    except Exception:
        raise last_err or Exception(f"Failed to read {path} with tried encodings.")

# -------------------------------------------------------------
# ğŸ“¦ ì¢Œí‘œ CSV ë³‘í•© ìœ í‹¸
# -------------------------------------------------------------
COORD_CSV_PATH = DATA_DIR / "ì„œìš¸ì‹œ_ì¬ê°œë°œì¬ê±´ì¶•_clean_kakao.csv"
COORD_ENCODING = "utf-8-sig"  # (ìŠ¤ë§ˆíŠ¸ ë¡œë”ê°€ ìë™íŒë³„í•˜ë¯€ë¡œ ì—†ì–´ë„ ë™ì‘)

# 1) CSV ë¡œë“œ ìœ í‹¸ (êµí†µëŸ‰ CSV)
@st.cache_data(show_spinner=False)
def load_volume_csv(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    df["link_id"] = df["link_id"].astype(str)

    # ì‹œê°„ ì•ˆì „ ì •ê·œí™”: "0ì‹œ", " 08 ", "8.0" ë“±ë„ 0~23ìœ¼ë¡œ ë³€í™˜
    h = pd.to_numeric(
        pd.Series(df["hour"]).astype(str).str.extract(r"(\d+)", expand=False),
        errors="coerce"
    ).fillna(0).astype(int) % 24
    df["hour"] = h

    df["ì°¨ëŸ‰ëŒ€ìˆ˜"] = pd.to_numeric(df["ì°¨ëŸ‰ëŒ€ìˆ˜"], errors="coerce").fillna(0)
    return df

# 2) êµí†µëŸ‰ ê°€ì¤‘ í˜¼ì¡ë¹ˆë„ê°•ë„(CFI) ê³„ì‚° (Hard threshold)
def compute_cfi_weighted(speed_df: pd.DataFrame, vol_df: pd.DataFrame, boundary_speed: float = 30.0):
    d = speed_df.copy()
    d["link_id"] = d["link_id"].astype(str)
    d["hour"] = d["hour"].astype(int)
    d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")

    # merge
    m = d.merge(vol_df, on=["link_id", "hour"], how="inner")

    # í˜¼ì¡ ì°¨ëŸ‰ìˆ˜(ì†ë„<=ê²½ê³„ê°’) vs ì „ì²´ ì°¨ëŸ‰ìˆ˜
    m["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] = (m["í‰ê· ì†ë„(km/h)"] <= boundary_speed).astype(int) * m["ì°¨ëŸ‰ëŒ€ìˆ˜"]
    g = m.groupby(["link_id", "hour"], as_index=False).agg(
        ì „ì²´ì°¨ëŸ‰ìˆ˜=("ì°¨ëŸ‰ëŒ€ìˆ˜", "sum"),
        í˜¼ì¡ì°¨ëŸ‰ìˆ˜=("í˜¼ì¡ì°¨ëŸ‰ìˆ˜", "sum"),
    )
    g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = (g["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] / g["ì „ì²´ì°¨ëŸ‰ìˆ˜"]).replace([float("inf"), float("nan")], 0) * 100
    return g

# 3) Soft(ì‹œê·¸ëª¨ì´ë“œ) CFI
def compute_cfi_soft(
    speed_df: pd.DataFrame,
    vol_df: pd.DataFrame,
    boundary_mode: str = "percentile",  # "percentile" or "fixed"
    boundary_value: float = 40.0,       # percentile: 10~90(%), fixed: km/h
    tau_kmh: float = 6.0                # ì‹œê·¸ëª¨ì´ë“œ ê¸‰ê²½ì‚¬ í­(ê°’ì´ í¬ë©´ ë” ë¶€ë“œëŸ¬ì›€)
):
    d = speed_df.copy()
    d["link_id"] = d["link_id"].astype(str)
    d["hour"] = pd.to_numeric(d["hour"], errors="coerce").astype("Int64")  # allow NA
    d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")

    v = vol_df.copy()
    v["link_id"] = v["link_id"].astype(str)
    v["hour"] = pd.to_numeric(v["hour"], errors="coerce").astype("Int64") % 24
    v["ì°¨ëŸ‰ëŒ€ìˆ˜"] = pd.to_numeric(v["ì°¨ëŸ‰ëŒ€ìˆ˜"], errors="coerce").fillna(0)

    m = d.merge(v, on=["link_id", "hour"], how="inner").dropna(subset=["í‰ê· ì†ë„(km/h)"])
    if m.empty:
        out = m[["link_id","hour"]].copy()
        out["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = 0.0
        out.attrs = {"boundary": np.nan, "mode": boundary_mode}
        return out

    if boundary_mode == "percentile":
        p = float(boundary_value)
        p = max(5.0, min(95.0, p))  # ì•ˆì „ ë²”ìœ„
        vb = float(np.nanpercentile(m["í‰ê· ì†ë„(km/h)"], p))
    else:
        vb = float(boundary_value)

    tau = max(1e-6, float(tau_kmh))
    m["p_cong"] = 1.0 / (1.0 + np.exp((m["í‰ê· ì†ë„(km/h)"] - vb) / tau))

    def _wavg(x, w):
        w = np.asarray(w)
        x = np.asarray(x)
        mask = np.isfinite(x) & np.isfinite(w) & (w >= 0)
        if not mask.any():
            return 0.0
        return float((x[mask] * w[mask]).sum() / max(1e-9, w[mask].sum()))

    g = (
        m.groupby(["link_id","hour"], as_index=False)
         .apply(lambda df: pd.Series({
             "í˜¼ì¡ë¹ˆë„ê°•ë„(%)": _wavg(df["p_cong"], df["ì°¨ëŸ‰ëŒ€ìˆ˜"]) * 100.0
         }))
         .reset_index()
    )

    g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"].clip(0, 100)
    g.attrs = {"boundary": vb, "mode": boundary_mode, "tau": tau}
    return g

@st.cache_data(show_spinner=False)
def load_coords() -> pd.DataFrame:
    df = smart_read_csv(COORD_CSV_PATH)

    # ìˆ«ìí™”
    df["lat"] = pd.to_numeric(df.get("lat"), errors="coerce")
    df["lon"] = pd.to_numeric(df.get("lon"), errors="coerce")

    # ì•ˆì „í•œ ë³‘í•©ìš© ê¸°ì´ˆ ì»¬ëŸ¼ ìƒì„±
    def _coal(a, b):
        a = "" if a is None else str(a).strip()
        b = "" if b is None else str(b).strip()
        return a if a else b

    # name, address ë§Œë“¤ê¸°
    if ("ì •ë¹„êµ¬ì—­ëª…ì¹­" in df.columns) or ("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…" in df.columns):
        name = [_coal(n, m) for n, m in zip(df.get("ì •ë¹„êµ¬ì—­ëª…ì¹­"), df.get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"))]
    else:
        name = df.get("name")

    address = [_coal(a, b) for a, b in zip(df.get("ì •ë¹„êµ¬ì—­ìœ„ì¹˜"), df.get("ëŒ€í‘œì§€ë²ˆ"))]

    out = pd.DataFrame({
        "apt_id": df.get("ì‚¬ì—…ë²ˆí˜¸").astype(str) if "ì‚¬ì—…ë²ˆí˜¸" in df.columns else "",
        "name": pd.Series(name, index=df.index),
        "gu": df.get("ìì¹˜êµ¬"),
        "address": pd.Series(address, index=df.index),
        "full_address": df.get("full_address"),
        "lat": df["lat"],
        "lon": df["lon"],
    })
    for col in ["apt_id", "name", "gu", "address", "full_address"]:
        if col in out.columns:
            out[col] = out[col].fillna("").astype(str).str.strip()
    return out

@st.cache_data(show_spinner=False)
def merge_projects_with_coords(gu: str) -> pd.DataFrame:
    # 1) ì›ë³¸ ë¡œë“œ + ìŠ¤í‚¤ë§ˆ í†µì¼
    raw = load_raw_csv()
    proj = normalize_schema(raw)
    proj = proj[proj["gu"] == gu].copy()

    # 2) ì¢Œí‘œ ë¡œë“œ
    coords = load_coords()
    coords = coords[coords["gu"] == gu].copy()

    # 3) âœ… name+gu ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­í•˜ê³  full_addressê¹Œì§€ ê°€ì ¸ì˜¤ê¸°
    out = proj.merge(
        coords[["name", "gu", "lat", "lon", "full_address"]],
        on=["name", "gu"],
        how="left"
    )

    # 4) ì¢Œí‘œ ê²°ì¸¡ ë³´ì • (êµ¬ ì¤‘ì‹¬ + ì§€í„°)
    missing = out["lat"].isna() | out["lon"].isna()
    base_lat, base_lon = GU_CENTER.get(gu, (37.55, 127.0))
    rng = np.random.default_rng(42)
    jitter = lambda n: rng.normal(0, 0.002, n)  # â‰ˆ 200m
    if missing.any():
        n = int(missing.sum())
        out.loc[missing, "lat"] = base_lat + jitter(n)
        out.loc[missing, "lon"] = base_lon + jitter(n)
    out["has_geo"] = ~missing

    # 5) âœ… í‘œì‹œìš© ì£¼ì†Œ: full_address ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸ address
    out["address_display"] = out["full_address"].fillna("").replace("", pd.NA)
    out["address_display"] = out["address_display"].fillna(out["address"]).fillna("")

    return out.reset_index(drop=True)

# -------------------------------------------------------------
# âš™ï¸ Streamlit ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------------------
st.set_page_config(
    page_title="AIoT ìŠ¤ë§ˆíŠ¸ ì¸í”„ë¼ ëŒ€ì‹œë³´ë“œ | ì¬ê±´ì¶• Helper",
    layout="wide",
)

st.markdown("""
<style>
/* ëª¨ë“  LaTeX ìˆ˜ì‹ì„ ì™¼ìª½ ì •ë ¬ë¡œ */
.katex-display { text-align: left !important; margin-left: 0 !important; }
/* í…ìŠ¤íŠ¸ ì „ì²´ ê¸°ë³¸ ì™¼ìª½ ì •ë ¬ ìœ ì§€ */
.block-container { text-align: left !important; }
</style>
""", unsafe_allow_html=True)

STYLE = """
<style>
.small-muted { color: #7a7a7a; font-size: 0.9rem; }
.kpi-card { padding: 16px; border-radius: 16px; background: #111827; border: 1px solid #1f2937; }
.kpi-title { font-size: 0.9rem; color: #9ca3af; margin-bottom: 8px; }
.kpi-value { font-size: 1.4rem; font-weight: 700; color: #e5e7eb; }
.section-title { font-size: 1.1rem; font-weight: 700; margin-bottom: 8px; }
hr.soft { border: none; height: 1px; background: #1f2937; margin: 16px 0; }
</style>
"""
st.markdown(STYLE, unsafe_allow_html=True)

st.markdown("""
<style>
div[data-testid="stMarkdownContainer"] .katex-display { text-align: left !important; margin-left: 0 !important; margin-right: auto !important; }
div[data-testid="stMarkdownContainer"] .katex-display > .katex { display: inline-block !important; }
div[data-testid="stMarkdownContainer"] p { text-align: left !important; margin-left: 0 !important; }
</style>
""", unsafe_allow_html=True)

# -------------------------------------------------------------
# ğŸ§¾ CSV ë¡œë“œ & ì „ì²˜ë¦¬
# -------------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_raw_csv() -> pd.DataFrame:
    return smart_read_csv(PROJECTS_CSV_PATH)  # âœ… ì¸ì½”ë”© ìë™ ê°ì§€ ì‚¬ìš©

def _coalesce(*vals):
    for v in vals:
        if pd.notna(v) and str(v).strip():
            return v
    return None

def normalize_schema(df_raw: pd.DataFrame) -> pd.DataFrame:
    c = df_raw.columns
    get = lambda name: df_raw[name] if name in c else pd.Series([None]*len(df_raw))

    def _num(x):
        return pd.to_numeric(pd.Series(x, index=df_raw.index), errors="coerce")

    def _pct_to_num(x):
        s = pd.Series(x, index=df_raw.index).astype(str).str.replace("%", "", regex=False)
        s = s.str.replace(",", "", regex=False)
        return pd.to_numeric(s, errors="coerce")

    def _floors_to_num(x):
        s = pd.Series(x, index=df_raw.index).astype(str).str.extract(r"(-?\d+)", expand=False)
        return pd.to_numeric(s, errors="coerce")

    # ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ë§¤í•‘
    df = pd.DataFrame({
        "apt_id": get("ì‚¬ì—…ë²ˆí˜¸"),
        "name": [v if (pd.notna(v) and str(v).strip()) else w for v, w in zip(get("ì •ë¹„êµ¬ì—­ëª…ì¹­"), get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"))],
        "org_name": get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"),
        "biz_type": get("ì‚¬ì—…êµ¬ë¶„"),
        "op_type": get("ìš´ì˜êµ¬ë¶„"),
        "gu": get("ìì¹˜êµ¬"),
        "address": [v if (pd.notna(v) and str(v).strip()) else w for v, w in zip(get("ì •ë¹„êµ¬ì—­ìœ„ì¹˜"), get("ëŒ€í‘œì§€ë²ˆ"))],
        "households": _num(get("ë¶„ì–‘ì„¸ëŒ€ì´ìˆ˜")),
        "land_area_m2": _num(get("ì •ë¹„êµ¬ì—­ë©´ì (ã¡)")),
        "far": _pct_to_num(get("ìš©ì ë¥ ")),
        "floors": _floors_to_num(get("ì¸µìˆ˜")),
        "status": get("ì§„í–‰ë‹¨ê³„"),
        "floors_up": _floors_to_num(get("ì§€ìƒì¸µìˆ˜")),
        "floors_down": _floors_to_num(get("ì§€í•˜ì¸µìˆ˜")),
    })

    def _fmt_floor(u, d):
        parts = []
        if pd.notna(u):
            parts.append(f"ì§€ìƒ {int(u)}")
        if pd.notna(d):
            parts.append(f"ì§€í•˜ {int(d)}")
        return " / ".join(parts)

    df["floors_display"] = [_fmt_floor(u, d) for u, d in zip(df["floors_up"], df["floors_down"])]
    df["floors_show"] = df["floors_display"].fillna("").astype(str).str.strip()
    mask_empty = df["floors_show"] == ""
    df.loc[mask_empty, "floors_show"] = df.loc[mask_empty, "floors"].apply(
        lambda x: f"{int(x)}ì¸µ" if pd.notna(x) else ""
    )

    df["apt_id"] = df["apt_id"].astype(str)
    df["name"] = df["name"].fillna("ë¬´ëª… ì •ë¹„êµ¬ì—­")
    for col in ["org_name","biz_type","op_type","gu","address","status"]:
        df[col] = df[col].fillna("").astype(str).str.strip()

    return df

@st.cache_data(show_spinner=False)
def get_projects_by_gu(gu: str) -> pd.DataFrame:
    raw = load_raw_csv()
    norm = normalize_schema(raw)
    return norm[norm["gu"] == gu].reset_index(drop=True)

# -------------------------------------------------------------
# ğŸ“ ì„œìš¸ì‹œ 25ê°œ ìì¹˜êµ¬ ë¦¬ìŠ¤íŠ¸ & ì¤‘ì‹¬ì¢Œí‘œ
# -------------------------------------------------------------
DISTRICTS = [
    "ê°•ë‚¨êµ¬","ê°•ë™êµ¬","ê°•ë¶êµ¬","ê°•ì„œêµ¬","ê´€ì•…êµ¬","ê´‘ì§„êµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬",
    "ë…¸ì›êµ¬","ë„ë´‰êµ¬","ë™ëŒ€ë¬¸êµ¬","ë™ì‘êµ¬","ë§ˆí¬êµ¬","ì„œëŒ€ë¬¸êµ¬","ì„œì´ˆêµ¬","ì„±ë™êµ¬",
    "ì„±ë¶êµ¬","ì†¡íŒŒêµ¬","ì–‘ì²œêµ¬","ì˜ë“±í¬êµ¬","ìš©ì‚°êµ¬","ì€í‰êµ¬","ì¢…ë¡œêµ¬","ì¤‘êµ¬","ì¤‘ë‘êµ¬"
]

GU_CENTER = {
    "ê°•ë‚¨êµ¬": (37.5172, 127.0473),
    "ê°•ë™êµ¬": (37.5301, 127.1238),
    "ê°•ë¶êµ¬": (37.6396, 127.0257),
    "ê°•ì„œêµ¬": (37.5509, 126.8495),
    "ê´€ì•…êµ¬": (37.4784, 126.9516),
    "ê´‘ì§„êµ¬": (37.5386, 127.0822),
    "êµ¬ë¡œêµ¬": (37.4955, 126.8876),
    "ê¸ˆì²œêµ¬": (37.4569, 126.8958),
    "ë…¸ì›êµ¬": (37.6543, 127.0565),
    "ë„ë´‰êµ¬": (37.6688, 127.0471),
    "ë™ëŒ€ë¬¸êµ¬": (37.5740, 127.0396),
    "ë™ì‘êµ¬": (37.5124, 126.9393),
    "ë§ˆí¬êµ¬": (37.5638, 126.9084),
    "ì„œëŒ€ë¬¸êµ¬": (37.5791, 126.9368),
    "ì„œì´ˆêµ¬": (37.4836, 127.0326),
    "ì„±ë™êµ¬": (37.5633, 127.0369),
    "ì„±ë¶êµ¬": (37.5894, 127.0167),
    "ì†¡íŒŒêµ¬": (37.5145, 127.1068),
    "ì–‘ì²œêµ¬": (37.5169, 126.8665),
    "ì˜ë“±í¬êµ¬": (37.5264, 126.8963),
    "ìš©ì‚°êµ¬": (37.5311, 126.9811),
    "ì€í‰êµ¬": (37.6176, 126.9227),
    "ì¢…ë¡œêµ¬": (37.5736, 126.9780),
    "ì¤‘êµ¬": (37.5636, 126.9976),
    "ì¤‘ë‘êµ¬": (37.6063, 127.0929),
}

def attach_latlon_by_gu_centroid(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["lat"] = df["gu"].map(lambda g: GU_CENTER.get(g, (37.55, 127.0))[0]) + np.random.normal(scale=0.004, size=len(df))
    df["lon"] = df["gu"].map(lambda g: GU_CENTER.get(g, (37.55, 127.0))[1]) + np.random.normal(scale=0.004, size=len(df))
    return df

# -------------------------------------------------------------
# ğŸ§­ ì‚¬ì´ë“œë°”
# -------------------------------------------------------------
st.sidebar.title("ì¬ê±´ì¶• ì˜ì‚¬ê²°ì • Helper")

# ì‚¬ì´ë“œë°” - êµ¬ ì„ íƒ
selected_gu = st.sidebar.selectbox("êµ¬ ì„ íƒ", DISTRICTS, index=0)

# âœ… ì„¸ì…˜ì— ì„ íƒ ì¸ë±ìŠ¤ ì´ˆê¸°í™”(ë§¨ ìœ„ í•œ ë²ˆ)
if "selected_row" not in st.session_state:
    st.session_state.selected_row = None
if "selected_gu_prev" not in st.session_state:
    st.session_state.selected_gu_prev = selected_gu

if st.session_state.selected_gu_prev != selected_gu:
    st.session_state.selected_row = None
    st.session_state.selected_gu_prev = selected_gu

st.sidebar.markdown(
    "<div class='small-muted'>êµ¬ ì„ íƒ ì‹œ, í•´ë‹¹ êµ¬ì˜ ì •ë¹„ì‚¬ì—… ë‹¨ì§€ ëª©ë¡ê³¼ ì§€ë„ê°€ ê°±ì‹ ë©ë‹ˆë‹¤.</div>",
    unsafe_allow_html=True,
)

project_name = st.sidebar.text_input("í”„ë¡œì íŠ¸ ì´ë¦„", value=f"{selected_gu} ì¬ê±´ì¶• ì‹œë‚˜ë¦¬ì˜¤")

# ì‚¬ì´ë“œë°” FAQ ë Œë”
render_sidebar_presets()
render_sidebar_4quadrant_guide()

# -------------------------------------------------------------
# ğŸ—ºï¸ 1-2ì‚¬ë¶„ë©´: ì§€ë„ + ë‹¨ì§€ ì„ íƒ
# -------------------------------------------------------------
col12_left, col12_right = st.columns([2.2, 1.4], gap="large")

with col12_left:
    st.markdown("### ğŸ—º [1-2ì‚¬ë¶„ë©´] Â· ì§€ë„ & ë‹¨ì§€ì„ íƒ")

    base_df = get_projects_by_gu(selected_gu)
    df_map = merge_projects_with_coords(selected_gu)

    # ì§€ë„ ìë¦¬ë§Œ ë¨¼ì € í™•ë³´ (í•„í„°/ì„ íƒ ì ìš© í›„ ì•„ë˜ì—ì„œ ì‹¤ì œ ì°¨íŠ¸ ë Œë”)
    map_slot = st.empty()

    if df_map.empty:
        st.warning("âš ï¸ í•´ë‹¹ êµ¬ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

# ---------------------------
# ğŸ“‹ ë‹¨ì§€ í…Œì´ë¸” + í•„í„° UI
# ---------------------------
with st.expander("ğŸ” ë°ì´í„° ì†ŒìŠ¤ í™•ì¸(ì„ì‹œ)", expanded=False):
    raw = load_raw_csv()
    cols_raw = ["ìì¹˜êµ¬", "ì •ë¹„êµ¬ì—­ëª…ì¹­", "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…", "ë¶„ì–‘ì„¸ëŒ€ì´ìˆ˜", "ì •ë¹„êµ¬ì—­ë©´ì (ã¡)"]
    exist_cols = [c for c in cols_raw if c in raw.columns]
    st.caption(f"ì›ë³¸: {PROJECTS_CSV_PATH.name} Â· í‘œì‹œì—´: {', '.join(exist_cols)}")
    try:
        st.dataframe(
            raw[exist_cols][raw["ìì¹˜êµ¬"] == selected_gu].head(20),
            use_container_width=True
        )
    except Exception:
        st.dataframe(raw[exist_cols].head(20), use_container_width=True)

    st.caption("í˜„ì¬ í‘œì‹œê°’(df_map)")
    st.dataframe(
        df_map[["gu", "address_display", "name", "households", "land_area_m2"]].head(20),
        use_container_width=True
    )

    def _coal2(a, b):
        a = "" if a is None else str(a).strip()
        b = "" if b is None else str(b).strip()
        return a if a else b

    raw_norm = pd.DataFrame({
        "gu": raw.get("ìì¹˜êµ¬"),
        "name_raw": [
            _coal2(n, m) for n, m in zip(
                raw.get("ì •ë¹„êµ¬ì—­ëª…ì¹­"),
                raw.get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…")
            )
        ],
        "households_src": pd.to_numeric(raw.get("ë¶„ì–‘ì„¸ëŒ€ì´ìˆ˜"), errors="coerce"),
        "land_area_m2_src": pd.to_numeric(raw.get("ì •ë¹„êµ¬ì—­ë©´ì (ã¡)"), errors="coerce"),
    })

    comp = df_map.merge(
        raw_norm, left_on=["name","gu"], right_on=["name_raw","gu"], how="left"
    )

    hh_match  = (comp["households"].fillna(-1).astype(float) == comp["households_src"].fillna(-1).astype(float))
    area_match = np.isclose(
        comp["land_area_m2"].astype(float),
        comp["land_area_m2_src"].astype(float),
        rtol=1e-6, atol=1e-6, equal_nan=True
    )

    comp["households_match"] = hh_match
    comp["land_area_match"]  = area_match

    total = len(comp)
    hh_ok  = int(hh_match.sum())
    ar_ok  = int(area_match.sum())

    st.write(
        f"âœ… ì„¸ëŒ€ìˆ˜ ì¼ì¹˜: **{hh_ok}/{total}**  Â·  "
        f"âœ… ë©´ì  ì¼ì¹˜: **{ar_ok}/{total}**"
    )

    bad = comp[~(hh_match & area_match)][
        ["gu","address_display","name","households","households_src","land_area_m2","land_area_m2_src"]
    ]
    if not bad.empty:
        st.warning("ë¶ˆì¼ì¹˜ ìƒ˜í”Œ(ìµœëŒ€ 20ê±´):")
        st.dataframe(bad.head(20), use_container_width=True)

st.markdown("**ë‹¨ì§€ ëª©ë¡**")

df_list = df_map[[
    "apt_id",
    "address_display",
    "org_name", "biz_type", "op_type",
    "status",
    "households", "land_area_m2",
    "far",
    "floors_show",
]].copy()

df_list["households"]   = pd.to_numeric(df_list["households"], errors="coerce")
df_list["land_area_m2"] = pd.to_numeric(df_list["land_area_m2"], errors="coerce")
df_list["far"]          = pd.to_numeric(df_list["far"], errors="coerce")

# ===== í•„í„° ì˜ì—­ =====
fcol1, fcol2, fcol3, fcol4 = st.columns([1.6, 1.4, 1.6, 1.2])
with fcol1:
    kw = st.text_input("ê²€ìƒ‰ì–´(ì£¼ì†Œ/ì¡°í•©ëª…/í‚¤ì›Œë“œ)", value="", placeholder="ì˜ˆ) ê°œí¬, ëª©ë™, ì¡°í•©")

HH_BUCKETS = {
    "ì „ì²´": (None, None),
    "~ 300ì„¸ëŒ€": (0, 300),
    "301â€“500ì„¸ëŒ€": (301, 500),
    "501â€“1,000ì„¸ëŒ€": (501, 1000),
    "1,001â€“2,000ì„¸ëŒ€": (1001, 2000),
    "2,001ì„¸ëŒ€ ì´ìƒ": (2001, None),
}

AREA_BUCKETS = {
    "ì „ì²´": (None, None),
    "~ 30,000 mÂ²": (0, 30000),
    "30,001â€“50,000 mÂ²": (30001, 50000),
    "50,001â€“100,000 mÂ²": (50001, 100000),
    "100,001â€“200,000 mÂ²": (100001, 200000),
    "200,001 mÂ² ì´ìƒ": (200001, None),
}

with fcol2:
    hh_choice = st.selectbox("ì„¸ëŒ€ìˆ˜ ë²”ì£¼", list(HH_BUCKETS.keys()), index=0)
with fcol3:
    la_choice = st.selectbox("ë©´ì  ë²”ì£¼(mÂ²)", list(AREA_BUCKETS.keys()), index=0)
with fcol4:
    hide_zero = st.checkbox("0/ê²°ì¸¡ì¹˜ ìˆ¨ê¸°ê¸°", value=True)

mask = pd.Series(True, index=df_list.index)

if kw.strip():
    _kw = kw.strip().lower()
    mask &= (
        df_list["address_display"].fillna("").str.lower().str.contains(_kw) |
        df_list["org_name"].fillna("").str.lower().str.contains(_kw)
    )

hh_lo, hh_hi = HH_BUCKETS[hh_choice]
la_lo, la_hi = AREA_BUCKETS[la_choice]

col_series = df_list["households"].fillna(-1)
if hh_lo is not None:
    mask &= col_series >= hh_lo
if hh_hi is not None:
    mask &= col_series <= hh_hi

col_series = df_list["land_area_m2"].fillna(-1)
if la_lo is not None:
    mask &= col_series >= la_lo
if la_hi is not None:
    mask &= col_series <= la_hi

if hide_zero:
    mask &= df_list["households"].fillna(0) > 0
    mask &= df_list["land_area_m2"].fillna(0) > 0

filtered = df_list[mask].reset_index().rename(columns={"index": "orig_index"})

scol1, scol2 = st.columns([1.2, 1.2])
with scol1:
    sort_key = st.selectbox("ì •ë ¬ ê¸°ì¤€",
        ["ì„¸ëŒ€ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ", "ì„¸ëŒ€ìˆ˜ ì˜¤ë¦„ì°¨ìˆœ", "ë©´ì  ë‚´ë¦¼ì°¨ìˆœ", "ë©´ì  ì˜¤ë¦„ì°¨ìˆœ", "ì£¼ì†Œ ì˜¤ë¦„ì°¨ìˆœ"], index=0)
with scol2:
    topn = st.selectbox("í‘œì‹œ ê°œìˆ˜", [10, 20, 50, 100, "ì „ì²´"], index=1)

if sort_key == "ì„¸ëŒ€ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ":
    filtered = filtered.sort_values("households", ascending=False, na_position="last")
elif sort_key == "ì„¸ëŒ€ìˆ˜ ì˜¤ë¦„ì°¨ìˆœ":
    filtered = filtered.sort_values("households", ascending=True, na_position="last")
elif sort_key == "ë©´ì  ë‚´ë¦¼ì°¨ìˆœ":
    filtered = filtered.sort_values("land_area_m2", ascending=False, na_position="last")
elif sort_key == "ë©´ì  ì˜¤ë¦„ì°¨ìˆœ":
    filtered = filtered.sort_values("land_area_m2", ascending=True, na_position="last")
elif sort_key == "ì£¼ì†Œ ì˜¤ë¦„ì°¨ìˆœ":
    filtered = filtered.sort_values("address_display", ascending=True, na_position="last")

if topn != "ì „ì²´":
    filtered = filtered.head(int(topn))

show_df = filtered[[
    "orig_index",
    "address_display", "org_name", "biz_type", "op_type",
    "status",
    "households", "land_area_m2",
    "far", "floors_show",
]].copy().rename(columns={
    "address_display": "ì£¼ì†Œ",
    "org_name": "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…",
    "biz_type": "ì‚¬ì—…êµ¬ë¶„",
    "op_type": "ìš´ì˜êµ¬ë¶„",
    "status": "ì§„í–‰ë‹¨ê³„",
    "households": "ê³„íšì„¸ëŒ€ìˆ˜",
    "land_area_m2": "ë©´ì ",
    "far": "ìš©ì ë¥ (%)",
    "floors_show": "ì¸µìˆ˜",
})

show_df.insert(1, "ì„ íƒ", False)

curr_ids = show_df["orig_index"].tolist()
if (st.session_state.selected_row is None) or (st.session_state.selected_row not in curr_ids):
    st.session_state.selected_row = int(curr_ids[0]) if curr_ids else None
show_df.loc[show_df["orig_index"] == st.session_state.selected_row, "ì„ íƒ"] = True

edited = st.data_editor(
    show_df,
    use_container_width=True,
    hide_index=True,
    disabled=[
        "orig_index", "ì£¼ì†Œ", "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…", "ì‚¬ì—…êµ¬ë¶„", "ìš´ì˜êµ¬ë¶„",
        "ì§„í–‰ë‹¨ê³„", "ê³„íšì„¸ëŒ€ìˆ˜", "ë©´ì ", "ìš©ì ë¥ (%)", "ì¸µìˆ˜"
    ],
    column_config={
        "orig_index": st.column_config.NumberColumn("ì›ë³¸ì¸ë±ìŠ¤", help="ë‚´ë¶€ ì„ íƒìš©", width="small"),
        "ì„ íƒ": st.column_config.CheckboxColumn("ì„ íƒ", help="ì´ í–‰ì„ ì„ íƒ"),
        "ì£¼ì†Œ": st.column_config.TextColumn("ì£¼ì†Œ"),
        "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…": st.column_config.TextColumn("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"),
        "ì‚¬ì—…êµ¬ë¶„": st.column_config.TextColumn("ì‚¬ì—…êµ¬ë¶„"),
        "ìš´ì˜êµ¬ë¶„": st.column_config.TextColumn("ìš´ì˜êµ¬ë¶„"),
        "ì§„í–‰ë‹¨ê³„": st.column_config.TextColumn("ì§„í–‰ë‹¨ê³„"),
        "ê³„íšì„¸ëŒ€ìˆ˜": st.column_config.NumberColumn("ê³„íšì„¸ëŒ€ìˆ˜", format="%,d"),
        "ë©´ì ": st.column_config.NumberColumn("ë©´ì (mÂ²)", format="%,d"),
        "ìš©ì ë¥ (%)": st.column_config.NumberColumn("ìš©ì ë¥ (%)", format=",.1f"),
        "ì¸µìˆ˜": st.column_config.TextColumn("ì¸µìˆ˜"),
    },
    key=f"project_table_{selected_gu}",
)

prev = st.session_state.selected_row
sel_list = [int(x) for x in edited.loc[edited["ì„ íƒ"] == True, "orig_index"].tolist()]

if len(sel_list) == 0:
    if prev in curr_ids:
        st.session_state.selected_row = int(prev)
    elif curr_ids:
        st.session_state.selected_row = int(curr_ids[0])
elif len(sel_list) == 1:
    st.session_state.selected_row = int(sel_list[0])
else:
    if prev in sel_list:
        new_choice = next((x for x in sel_list if x != prev), sel_list[0])
    else:
        new_choice = sel_list[0]
    st.session_state.selected_row = int(new_choice)
    st.rerun()

if st.session_state.selected_row is None:
    st.info("ì„ íƒëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()
selected_row = st.session_state.selected_row

# âœ… 4ì‚¬ë¶„ë©´ ì—°ë™ìš© ì„¸ì…˜ ì„¤ì •
selected_site_name = df_map.loc[selected_row, "name"]
st.session_state["selected_site"] = selected_site_name

# âœ… ì§€ë„ ë°ì´í„°/ë ˆì´ì–´ ë§Œë“¤ê¸° (selected_row í™•ì • ì´í›„)
filtered_indices = edited["orig_index"].tolist()
map_data = df_map.loc[filtered_indices].reset_index(drop=True)

def _point_tooltip(row):
    addr = row.get("address_display", "")
    gu = row.get("gu", "")
    hh = row.get("households", "")
    la = row.get("land_area_m2", "")
    return (f"<b>{addr}</b><br/>"
            f"ìì¹˜êµ¬: {gu}<br/>"
            f"ì„¸ëŒ€ìˆ˜: {hh}<br/>"
            f"êµ¬ì—­ë©´ì (mÂ²): {la}")

map_data = map_data.copy()
map_data["tooltip_html"] = map_data.apply(_point_tooltip, axis=1)

highlight_row = df_map.loc[[selected_row]].assign(_selected=True)
highlight_row = highlight_row.copy()
highlight_row["tooltip_html"] = highlight_row.apply(_point_tooltip, axis=1)

sel_lat = float(df_map.loc[selected_row, "lat"])
sel_lon = float(df_map.loc[selected_row, "lon"])
view_state = pdk.ViewState(latitude=sel_lat, longitude=sel_lon, zoom=12.5)

layer_points = pdk.Layer(
    "ScatterplotLayer",
    data=map_data,
    get_position='[lon, lat]',
    get_radius=60,
    pickable=True,
    get_fill_color=[255, 140, 0, 160],
    get_line_color=[255, 255, 255],
    line_width_min_pixels=0.5,
)

layer_highlight = pdk.Layer(
    "ScatterplotLayer",
    data=highlight_row,
    get_position='[lon, lat]',
    get_radius=150,
    pickable=False,
    get_fill_color=[0, 200, 255, 220],
    get_line_color=[0, 0, 0],
    line_width_min_pixels=1.2,
)

tooltip = {
    "html": "{tooltip_html}",
    "style": {"backgroundColor": "#0f172a", "color": "white"},
}

with col12_right:
    st.markdown("### ğŸ§¾ [1ì‚¬ë¶„ë©´] Â· ê¸°ì¡´ ë‹¨ì§€ ì •ë³´")
    current = df_map.loc[selected_row]
    with st.container(border=True):
        st.markdown("**ê¸°ì¡´ ë‹¨ì§€ ì •ë³´**")
        st.markdown(
            f"- ì£¼ì†Œ: **{current['address_display']}**\n\n"
            f"- ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…: **{current['org_name']}**\n\n"
            f"- ìì¹˜êµ¬: **{current['gu']}**\n\n"
            f"- ê³„íš ì„¸ëŒ€ìˆ˜: **{int(current['households']) if pd.notna(current['households']) else 'ë¯¸ìƒ'} ì„¸ëŒ€**\n\n"
            f"- ì •ë¹„êµ¬ì—­ë©´ì : **{int(current['land_area_m2']):,} mÂ²**"
        )

# 3â€“4ì‚¬ë¶„ë©´ ë ˆì´ì•„ì›ƒ ì»¬ëŸ¼
col3, col4 = st.columns([1.6, 1.4], gap="large")

# === 3ì‚¬ë¶„ë©´: í˜¼ì¡ë„ ê·¸ë˜í”„ ===
with st.spinner("êµí†µ ê¸°ì¤€ë…„ë„ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
    if TRAFFIC_XLSX_PATH.exists():
        ensure_speed_csv(TRAFFIC_XLSX_PATH, TRAFFIC_CSV_PATH)
    elif not TRAFFIC_CSV_PATH.exists():
        st.warning(f"ê¸°ì¤€ CSVê°€ ì—†ìŠµë‹ˆë‹¤: {TRAFFIC_CSV_PATH.name}\n"
                   f"â†’ data í´ë”ì— {TRAFFIC_XLSX_PATH.name} ë¥¼ ë„£ìœ¼ë©´ ìë™ ë³€í™˜ë©ë‹ˆë‹¤.")

sel_lat = float(current.get("lat", 37.5667))
sel_lon = float(current.get("lon", 126.9784))

def _to_norm_str_id(s):
    return (
        pd.Series(s, dtype="object")
          .astype(str)
          .str.replace(r"\.0$", "", regex=True)
          .str.strip()
    )

with col3:
    st.markdown("### ğŸš¦ [3-1ì‚¬ë¶„ë©´] Â· ì£¼ë³€ ë„ë¡œ í˜¼ì¡ë„ (ê¸°ì¤€ë…„ë„)")

    radius = st.slider("ë°˜ê²½(m)", 500, 3000, 1000, step=250, key="radius_m")
    max_links = st.slider("í‘œì‹œ ë§í¬ ìˆ˜", 5, 20, 10, step=1, key="max_links")

    df_plot = None
    if TRAFFIC_CSV_PATH.exists() and SHP_PATH.exists():
        if _HAS_PLOT_SPEED is True and _plot_speed is not None:
            chart_or_fig, df_plot = _plot_speed(
                csv_path=TRAFFIC_CSV_PATH,
                shp_path=SHP_PATH,
                center_lon=sel_lon,
                center_lat=sel_lat,
                radius_m=radius,
                max_links=max_links,
                renderer="altair",
                chart_height=700,
            )
            if isinstance(chart_or_fig, alt.Chart):
                st.altair_chart(chart_or_fig, use_container_width=True, theme=None)
            else:
                st.pyplot(chart_or_fig, use_container_width=True)
        elif _HAS_PLOT_SPEED is False and _plot_nearby is not None:
            fig, df_plot = _plot_nearby(
                csv_path=TRAFFIC_CSV_PATH,
                shp_path=SHP_PATH,
                center_lon=sel_lon,
                center_lat=sel_lat,
                radius_m=radius,
                max_links=max_links,
            )
            st.pyplot(fig, use_container_width=True)
        else:
            st.info("`utils/traffic_plot.py` ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê°„ëµ ëª¨ë“œë¡œ í‘œì‹œí•©ë‹ˆë‹¤. (ì‹œê°í™” ìƒëµ)")
    else:
        st.info("êµí†µ CSV ë˜ëŠ” SHPê°€ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    if df_plot is not None:
        with st.expander("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
            st.dataframe(
                df_plot.sort_values(["link_id", "hour"]).head(300),
                use_container_width=True
            )

    # === 3ì‚¬ë¶„ë©´: ì‹œê°„ëŒ€ ê¸°ì¤€ ê²°ê³¼ â†’ SHP ë§¤ì¹­ â†’ GeoJSON(ì‹œê°„ëŒ€) ì €ì¥ ===
    try:
        if df_plot is not None and not df_plot.empty:
            shp = gpd.read_file(SHP_PATH)[[LINK_ID_COL, "geometry"]]
            if shp.crs is None or (shp.crs.to_epsg() != 4326):
                shp = shp.to_crs(epsg=4326)

            shp["link_id_norm"] = (
                pd.to_numeric(shp[LINK_ID_COL], errors="coerce").round().astype("Int64").astype(str)
            )
            ids = (
                df_plot["link_id"].astype(str).str.replace(r"\.0$", "", regex=True).unique().tolist()
            )
            link_gdf = shp[shp["link_id_norm"].isin(ids)].copy()

            st.session_state["matched_links_geojson"] = link_gdf.__geo_interface__
        else:
            st.session_state["matched_links_geojson"] = None
    except Exception as e:
        st.session_state["matched_links_geojson"] = None
        st.info(f"ë§í¬ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {e}")

    # === í˜¼ì¡ë„ / í˜¼ì¡ë¹ˆë„ê°•ë„ í† ê¸€ ê·¸ë˜í”„ ===
    if 'df_plot' in locals() and df_plot is not None and not df_plot.empty:
        st.markdown("### ğŸ“ˆ [3-2ì‚¬ë¶„ë©´] í˜¼ì¡ì§€í‘œ ë¹„êµ (í˜¼ì¡ë„ vs í˜¼ì¡ë¹ˆë„ê°•ë„)")

        def compute_congestion_from_speed(df_plot):
            d = df_plot.copy()
            d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")
            d["free_flow"] = d.groupby("link_id")["í‰ê· ì†ë„(km/h)"].transform("max").clip(lower=1)
            d["í˜¼ì¡ë„(%)"] = ((1 - (d["í‰ê· ì†ë„(km/h)"] / d["free_flow"]).clip(0, 1)) * 100).clip(0, 100)
            d["ì§€í‘œëª…"] = "í˜¼ì¡ë„"
            return d[["link_id", "hour", "í˜¼ì¡ë„(%)", "ì§€í‘œëª…"]]

        def compute_congestion_freq_intensity(df_plot, boundary_speed=30):
            d = df_plot.copy()
            d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")
            d["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = (d["í‰ê· ì†ë„(km/h)"] <= boundary_speed).astype(int) * 100
            d = d.groupby(["link_id", "hour"], as_index=False)["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"].mean()
            d["ì§€í‘œëª…"] = "í˜¼ì¡ë¹ˆë„ê°•ë„"
            return d

        def compute_cfi_weighted_robust(
                speed_df: pd.DataFrame,
                vol_df: pd.DataFrame,
                boundary_mode: str = "percentile",
                boundary_value: float = 30.0,
                min_samples: int = 1
        ):
            d = speed_df.copy()
            d["link_id"] = d["link_id"].astype(str)
            d["hour"] = pd.to_numeric(d["hour"], errors="coerce").astype("Int64")
            d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")

            v = vol_df.copy()
            v["link_id"] = v["link_id"].astype(str)
            v["hour"] = pd.to_numeric(v["hour"], errors="coerce").astype("Int64") % 24
            v["ì°¨ëŸ‰ëŒ€ìˆ˜"] = pd.to_numeric(v["ì°¨ëŸ‰ëŒ€ìˆ˜"], errors="coerce").fillna(0)

            m = d.merge(v, on=["link_id", "hour"], how="inner").dropna(subset=["í‰ê· ì†ë„(km/h)"])

            if boundary_mode == "percentile":
                p = float(boundary_value)
                p = max(5.0, min(95.0, p))
                boundary = np.nanpercentile(m["í‰ê· ì†ë„(km/h)"], p)
            else:
                boundary = float(boundary_value)

            m["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] = (m["í‰ê· ì†ë„(km/h)"] <= boundary).astype(int) * m["ì°¨ëŸ‰ëŒ€ìˆ˜"]

            g = (m.groupby(["link_id", "hour"], as_index=False)
                 .agg(ì „ì²´ì°¨ëŸ‰ìˆ˜=("ì°¨ëŸ‰ëŒ€ìˆ˜", "sum"),
                      í˜¼ì¡ì°¨ëŸ‰ìˆ˜=("í˜¼ì¡ì°¨ëŸ‰ìˆ˜", "sum")))

            g.loc[g["ì „ì²´ì°¨ëŸ‰ìˆ˜"] < max(1, min_samples), ["í˜¼ì¡ì°¨ëŸ‰ìˆ˜", "ì „ì²´ì°¨ëŸ‰ìˆ˜"]] = np.nan
            g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = (g["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] / g["ì „ì²´ì°¨ëŸ‰ìˆ˜"]) * 100
            g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"].fillna(0).clip(0, 100)

            g.attrs = {"boundary": boundary, "mode": boundary_mode}
            return g

        metric_choice = st.radio(
            "í‘œì‹œí•  í˜¼ì¡ì§€í‘œ ì„ íƒ",
            ["í˜¼ì¡ë„", "í˜¼ì¡ë¹ˆë„ê°•ë„"],
            horizontal=True,
            index=0,
            key="metric_toggle"
        )

        if metric_choice == "í˜¼ì¡ë„":
            df_metric = compute_congestion_from_speed(df_plot).rename(columns={"í˜¼ì¡ë„(%)": "value"})
            y_title = "í˜¼ì¡ë„ (0=ììœ ì£¼í–‰, 100=ë§¤ìš°í˜¼ì¡)"
        else:
            vol_path = DATA_DIR / "TrafficVolume_Seoul_2023.csv"
            if vol_path.exists():
                vol_norm = load_volume_csv(vol_path)
                bcol1, bcol2, bcol3 = st.columns([1, 1, 1])
                with bcol1:
                    boundary_mode = st.radio("ê²½ê³„ë°©ì‹", ["percentile", "fixed"], horizontal=True, index=0, key="bd_mode")
                with bcol2:
                    if boundary_mode == "percentile":
                        boundary_value = float(st.slider("ì†ë„ë¶„í¬ ë¶„ìœ„ìˆ˜(%)", 10, 90, 40, 5, key="bd_pct"))
                    else:
                        boundary_value = float(st.number_input("ê³ ì • ê²½ê³„ì†ë„(km/h)", 10.0, 100.0, 30.0, 1.0, key="bd_fix"))
                with bcol3:
                    band_kmh = float(st.slider("ì™„í™” ë°´ë“œí­ (km/h)", 5, 20, 10, 1, key="bd_band"))

                df_cfi = compute_cfi_soft(
                    df_plot, vol_norm,
                    boundary_mode=boundary_mode,
                    boundary_value=boundary_value,
                    tau_kmh=band_kmh
                )
                used_boundary = getattr(df_cfi, "attrs", {}).get("boundary", None)
                if used_boundary is not None:
                    st.caption(f"ì‚¬ìš©ëœ ê²½ê³„ì†ë„ â‰ˆ {used_boundary:.1f} km/h (ë°´ë“œí­ {band_kmh:.1f} km/h)")

                df_metric = df_cfi.rename(columns={"í˜¼ì¡ë¹ˆë„ê°•ë„(%)": "value"})
                y_title = "í˜¼ì¡ë¹ˆë„ê°•ë„ (êµí†µëŸ‰ ê°€ì¤‘ Â· Soft)"
            else:
                df_metric = compute_congestion_freq_intensity(df_plot).rename(columns={"í˜¼ì¡ë¹ˆë„ê°•ë„(%)": "value"})
                y_title = "í˜¼ì¡ë¹ˆë„ê°•ë„ (í˜¼ì¡êµ¬ê°„ ì°¨ëŸ‰ë¹„ìœ¨)"

        CHART_H = 400
        HALF_W = 1100

        chart = (
            alt.Chart(df_metric)
            .mark_line(point=True)
            .encode(
                x=alt.X("hour:Q", title="ì‹œê°„ëŒ€ (ì‹œ)"),
                y=alt.Y("value:Q", title=y_title, scale=alt.Scale(domain=[0, 100])),
                color=alt.Color(
                    "link_id:N",
                    title="ë§í¬ ID",
                    legend=alt.Legend(orient="bottom", direction="horizontal", columns=4),
                ),
                tooltip=[
                    alt.Tooltip("link_id:N", title="ë§í¬"),
                    alt.Tooltip("hour:Q", title="ì‹œ"),
                    alt.Tooltip("value:Q", title=y_title, format=".1f"),
                ],
            )
            .properties(title=f"{metric_choice} ë³€í™” ì¶”ì´", width=HALF_W, height=CHART_H)
            .configure_view(strokeWidth=0)
        )
        st.altair_chart(chart, use_container_width=False, theme=None)

        if metric_choice == "í˜¼ì¡ë„":
            st.markdown("### ğŸ§® í˜¼ì¡ë„(%) ì •ì˜")
            st.markdown("- ë§í¬ $(l)$, ì‹œê°„ëŒ€ $(h)$ì—ì„œì˜ í‰ê· ì†ë„ë¥¼ $v_{l,h}$ ë¼ í•  ë•Œ,")
            st.latex(r"v_{\mathrm{ff},l}=\max v_{l,h}")
            st.latex(r"\mathrm{í˜¼ì¡ë„}_{l,h}(\%)=\Big(1-\min\big(1,\frac{v_{l,h}}{v_{\mathrm{ff},l}}\big)\Big)\times 100")
            st.markdown("- ê°’ì˜ ì˜ë¯¸: **0% = ììœ ì£¼í–‰**, **100% = ë§¤ìš° í˜¼ì¡**")
        else:
            st.markdown("### ğŸ§® í˜¼ì¡ë¹ˆë„ê°•ë„(%) ì •ì˜ (êµí†µëŸ‰ ê°€ì¤‘ Â· Soft)")
            st.markdown("- ê²½ê³„ì†ë„ $v_b$ ë¶€ê·¼ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜ë˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í™•ë¥ ë¡œ í˜¼ì¡ ì—¬ë¶€ë¥¼ ê·¼ì‚¬í•©ë‹ˆë‹¤.")
            st.latex(r"p_{\mathrm{cong}}(v)=\frac{1}{1+\exp\!\left(\frac{v-v_b}{\tau}\right)}")
            st.markdown("- ë§í¬Â·ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë¹ˆë„ê°•ë„ëŠ” **êµí†µëŸ‰ ê°€ì¤‘ í‰ê· **ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.")
            st.latex(r"""\mathrm{CFI}_{l,h}(\%)=100\times
            \frac{\sum_i w_{l,h,i}\,p_{\mathrm{cong}}(v_{l,h,i})}{\sum_i w_{l,h,i}}""")
            st.markdown("- ì—¬ê¸°ì„œ $w$ëŠ” ì°¨ëŸ‰ëŒ€ìˆ˜, $\\tau$ëŠ” ì „í™˜ì˜ ë¶€ë“œëŸ¬ì›€ì„ ì œì–´í•˜ëŠ” ë°´ë“œí­(km/h)ì…ë‹ˆë‹¤.")

        # === [MAKE DAILY GEOJSON] 3ì‚¬ë¶„ë©´ df_metric â†’ ì¼í‰ê·  GeoJSON ì €ì¥ (ë Œë” ê¸ˆì§€) ===
        def _color_from_value(v):
            if pd.isna(v): return (200, 200, 200)
            v = float(v)
            if v < 30:   return (0, 200, 0)       # ì´ˆë¡
            if v < 70:   return (255, 200, 0)     # ë…¸ë‘
            return (255, 0, 0)                    # ë¹¨ê°•

        try:
            if df_metric is not None and not df_metric.empty and SHP_PATH.exists():
                # 1) ì¼í‰ê·  ì§‘ê³„
                df_daily = (
                    df_metric.groupby("link_id", as_index=False)["value"]
                             .mean()
                             .rename(columns={"value": "daily_value"})
                )
                df_daily["link_id_norm"] = _to_norm_str_id(df_daily["link_id"])

                # 2) SHP ë§¤ì¹­
                gdf_link = gpd.read_file(SHP_PATH)[[LINK_ID_COL, "geometry"]]
                if gdf_link.crs and gdf_link.crs.to_epsg() != 4326:
                    gdf_link = gdf_link.to_crs(epsg=4326)
                gdf_link["link_id_norm"] = (
                    pd.to_numeric(gdf_link[LINK_ID_COL], errors="coerce").round().astype("Int64").astype(str)
                )
                gdf_vis = gdf_link.merge(df_daily, on="link_id_norm", how="inner")

                if not gdf_vis.empty:
                    # 3) GeoJSON propertiesì— ìƒ‰ìƒê°’ ì¶”ê°€
                    cols = list(zip(*gdf_vis["daily_value"].apply(_color_from_value)))
                    gdf_vis["color_r"], gdf_vis["color_g"], gdf_vis["color_b"] = cols[0], cols[1], cols[2]

                    # ë§í¬ ì „ìš© íˆ´íŒ HTML
                    gdf_vis["tooltip_html"] = (
                        "<b>ë§í¬:</b> " + gdf_vis["link_id_norm"].astype(str) +
                        "<br/><b>ì¼í‰ê·  í˜¼ì¡ë„:</b> " + gdf_vis["daily_value"].round(1).astype(str) + "%"
                    )

                    # 4) ì„¸ì…˜ ì €ì¥ (ì¼í‰ê· )
                    st.session_state["matched_links_geojson_daily"] = gdf_vis.__geo_interface__
                else:
                    st.session_state["matched_links_geojson_daily"] = None
        except Exception as e:
            st.session_state["matched_links_geojson_daily"] = None
            st.info(f"ì¼í‰ê·  í˜¼ì¡ë„ GeoJSON ìƒì„± ì˜¤ë¥˜: {e}")

# ================================================================
# ğŸ—ºï¸ 1â€“2ì‚¬ë¶„ë©´ ë‹¨ì¼ ë Œë” ë¸”ë¡ (daily â†’ hourly â†’ points/highlight)
# ================================================================
with col12_left:
    layers = []
    # 1) ì¼í‰ê·  í˜¼ì¡ë„ GeoJSON ìš°ì„ 
    gj_daily = st.session_state.get("matched_links_geojson_daily")
    if gj_daily:
        layer_links = pdk.Layer(
            "GeoJsonLayer",
            data=gj_daily,
            pickable=True,
            auto_highlight=True,
            get_line_color='[properties.color_r, properties.color_g, properties.color_b, 220]',
            lineWidthMinPixels=3,
        )
        layers.append(layer_links)
    else:
        # 2) ì—†ìœ¼ë©´ ì‹œê°„ëŒ€ ê¸°ì¤€ GeoJSON
        gj_hourly = st.session_state.get("matched_links_geojson")
        if gj_hourly:
            layer_links = pdk.Layer(
                "GeoJsonLayer",
                data=gj_hourly,
                pickable=True,
                auto_highlight=True,
                get_line_color=[255, 80, 80, 220],
                lineWidthMinPixels=3,
            )
            layers.append(layer_links)

    # 3) ê¸°ë³¸ ì /í•˜ì´ë¼ì´íŠ¸ëŠ” í•­ìƒ ì¶”ê°€
    layers += [layer_points, layer_highlight]

    map_slot.pydeck_chart(
        pdk.Deck(
            layers=layers,
            initial_view_state=view_state,
            tooltip=tooltip,
        )
    )

# -------------------------------------------------------------
# ğŸ’¡ 4ì‚¬ë¶„ë©´ Â· ì‹œë‚˜ë¦¬ì˜¤/ì¬ë¬´/ë¯¼ê°ë„/ë¦¬í¬íŠ¸ (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)
# -------------------------------------------------------------
with col4:
    st.markdown("### ğŸ§¾ [4ì‚¬ë¶„ë©´] Â· ì‹œë‚˜ë¦¬ì˜¤ & ì¬ë¬´/ë¯¼ê°ë„ & ë¦¬í¬íŠ¸")

    # ---------------------------
    # 0) ê³µí†µ ìœ í‹¸
    # ---------------------------
    def simple_npv(rate: float, cashflows):
        """ t=1ë¶€í„° í• ì¸í•˜ëŠ” ë‹¨ìˆœ NPV (ì–µì› ë‹¨ìœ„ cashflows ê°€ì •) """
        return float(sum(cf / ((1 + rate) ** t) for t, cf in enumerate(cashflows, start=1)))

    def calc_kpis(
        households:int,
        avg_py:float,                 # ì „ìš©í‰í˜•(í‰)
        sale_price_per_m2:float,      # ë¶„ì–‘ê°€ (ë§Œì›/ã¡)
        build_cost_per_m2:float,      # ê³µì‚¬ë¹„ (ë§Œì›/ã¡)
        infra_invest_billion:float,   # êµí†µ ë“± ì¸í”„ë¼ íˆ¬ì(ì–µì›)
        congestion_base:float,        # ê¸°ì¤€ í˜¼ì¡ë„(%)
        bus_inc_pct:int,              # ë²„ìŠ¤ ì¦í¸(%)
        non_sale_ratio:float=0.15,    # ë¹„ë¶„ì–‘ ë¹„ìœ¨(ê³µê³µ/ì»¤ë®¤ë‹ˆí‹° ë“±)
        sale_rate:float=0.98,         # ë¶„ì–‘ë¥ 
        disc_rate:float=0.07,         # í• ì¸ìœ¨
        years:int=4                   # íšŒìˆ˜ê¸°ê°„(ë…„)
    ):
        # ë©´ì  í™˜ì‚°
        m2_per_py = 3.3058
        avg_m2 = avg_py * m2_per_py
        sellable_m2 = households * avg_m2 * (1 - non_sale_ratio)  # ë¶„ì–‘ë©´ì 

        # í˜¼ì¡ë„ ê°œì„  (ê°„ì´ ëª¨ë¸)
        predicted_cong = max(0.0, congestion_base * (1 - bus_inc_pct / 150))
        cong_improve = max(0.0, congestion_base - predicted_cong)

        # ë§¤ì¶œ/ë¹„ìš© (ë§Œì› ë‹¨ìœ„ -> ì–µì› í™˜ì‚°)
        revenue_won = sellable_m2 * sale_price_per_m2 * sale_rate  # (ë§Œì›)
        cost_won = sellable_m2 * build_cost_per_m2                 # (ë§Œì›)
        total_cost_bil = cost_won/1e4/100 + infra_invest_billion   # (ì–µì›)
        total_rev_bil  = revenue_won/1e4/100                       # (ì–µì›)

        profit_bil = total_rev_bil - total_cost_bil
        margin_pct = (profit_bil / total_cost_bil * 100) if total_cost_bil>0 else 0

        # ê°„ì´ NPV (ê· ë“±í˜„ê¸ˆíë¦„ ê°€ì •)
        cf_annual = profit_bil / years
        npv = simple_npv(disc_rate, [cf_annual] * years)  # ğŸ‘ˆ numpy_financial ì—†ì´ ê³„ì‚°
        payback = min(years, max(1, int(np.ceil(total_cost_bil / max(1e-6, cf_annual)))))

        return {
            "ë¶„ì–‘ë©´ì (ã¡)": sellable_m2,
            "ì˜ˆìƒí˜¼ì¡ë„(%)": round(predicted_cong,1),
            "í˜¼ì¡ë„ê°œì„ (Î”%)": round(cong_improve,1),
            "ì´ë§¤ì¶œ(ì–µì›)": round(total_rev_bil,1),
            "ì´ì‚¬ì—…ë¹„(ì–µì›)": round(total_cost_bil,1),
            "ì´ìµ(ì–µì›)": round(profit_bil,1),
            "ë§ˆì§„ìœ¨(%)": round(margin_pct,1),
            "NPV(ì–µì›)": round(npv,1),
            "íšŒìˆ˜ê¸°ê°„(ë…„)": payback,
        }

    # ---------------------------
    # 1) ì…ë ¥/ì‹œë‚˜ë¦¬ì˜¤ íƒ­
    # ---------------------------
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ§© ì…ë ¥Â·ì‹œë‚˜ë¦¬ì˜¤", "ğŸ“ˆ ë¯¼ê°ë„", "ğŸ² í™•ë¥ (ê°„ì´)", "ğŸ“¤ ë¦¬í¬íŠ¸"])

    with tab1:
        st.markdown("#### ğŸ“‹ ê³µí†µ ì…ë ¥")
        c1, c2 = st.columns(2)
        with c1:
            households = int(st.number_input("ê³„íš ì„¸ëŒ€ìˆ˜", 100, 10000, int(current.get("households") or 1000), step=50))
            avg_py = st.number_input("í‰ê·  ì „ìš©ë©´ì (í‰)", 10.0, 60.0, float(st.session_state.get("desired_py", 25.0)), 0.5)
            congestion_base = st.number_input("ê¸°ì¤€ í˜¼ì¡ë„(%)", 0.0, 100.0, 50.0, 1.0)
            non_sale_ratio = st.slider("ë¹„ë¶„ì–‘ ë¹„ìœ¨", 0.0, 0.4, 0.15, 0.01, help="ê³µê³µ/ì»¤ë®¤ë‹ˆí‹° ë“±")
        with c2:
            sale_rate = st.slider("ë¶„ì–‘ë¥ ", 0.80, 1.00, 0.98, 0.01)
            disc_rate = st.slider("í• ì¸ìœ¨(ì¬ë¬´)", 0.03, 0.15, 0.07, 0.005)
            years = st.slider("íšŒìˆ˜ê¸°ê°„(ë…„)", 2, 10, 4, 1)
            base_bus_inc = st.slider("ë² ì´ìŠ¤ë¼ì¸ ë²„ìŠ¤ ì¦í¸(%)", 0, 100, 10, 5)

        st.markdown("#### ğŸ§ª ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜")
        st.caption("ë¶„ì–‘ê°€/ê³µì‚¬ë¹„/ë²„ìŠ¤ì¦í¸/ì¸í”„ë¼íˆ¬ìë§Œ ë‹¤ë¥´ê²Œ í•˜ë©°, ë‚˜ë¨¸ì§€ëŠ” ê³µí†µ ì…ë ¥ì„ ìƒì†í•©ë‹ˆë‹¤.")
        def scenario_inputs(label, sale_default, cost_default, bus_default, infra_default):
            a, b, c, d = st.columns(4, gap="small")
            with a:
                sale = st.number_input(f"{label}Â·ë¶„ì–‘ê°€(ë§Œì›/ã¡)", 500.0, 3000.0, sale_default, 10.0, key=f"sale_{label}")
            with b:
                cost = st.number_input(f"{label}Â·ê³µì‚¬ë¹„(ë§Œì›/ã¡)", 300.0, 2500.0, cost_default, 10.0, key=f"cost_{label}")
            with c:
                bus = st.slider(f"{label}Â·ë²„ìŠ¤ì¦í¸(%)", 0, 100, bus_default, 5, key=f"bus_{label}")
            with d:
                infra = st.number_input(f"{label}Â·ì¸í”„ë¼(ì–µì›)", 0.0, 1000.0, infra_default, 5.0, key=f"infra_{label}")
            return sale, cost, bus, infra

        saleA, costA, busA, infraA = scenario_inputs("A", 1200.0, 900.0, base_bus_inc, 30.0)
        saleB, costB, busB, infraB = scenario_inputs("B", 1300.0, 950.0, base_bus_inc+10, 50.0)
        saleC, costC, busC, infraC = scenario_inputs("C", 1100.0, 850.0, max(0, base_bus_inc-5), 20.0)

        scenarios = {
            "A": dict(sale=saleA, cost=costA, bus=busA, infra=infraA),
            "B": dict(sale=saleB, cost=costB, bus=busB, infra=infraB),
            "C": dict(sale=saleC, cost=costC, bus=busC, infra=infraC),
        }

        rows = []
        for name, s in scenarios.items():
            k = calc_kpis(
                households, avg_py, s["sale"], s["cost"], s["infra"],
                congestion_base, s["bus"], non_sale_ratio, sale_rate, disc_rate, years
            )
            rows.append({"ì‹œë‚˜ë¦¬ì˜¤": name, **k})
        df_scn = pd.DataFrame(rows).set_index("ì‹œë‚˜ë¦¬ì˜¤")

        st.markdown("#### ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµí‘œ")
        st.dataframe(df_scn, use_container_width=True)

        # í•˜ì´ë¼ì´íŠ¸ ì¹´ë“œ
        best = df_scn.sort_values("NPV(ì–µì›)", ascending=False).head(1)
        st.success(f"**ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤: {best.index[0]}** Â· NPV {best['NPV(ì–µì›)'].iloc[0]:,.1f}ì–µì› Â· ë§ˆì§„ìœ¨ {best['ë§ˆì§„ìœ¨(%)'].iloc[0]:.1f}%")

    # ---------------------------
    # 2) ë¯¼ê°ë„ (í† ë„¤ì´ë„ ì°¨íŠ¸)
    # ---------------------------
    with tab2:
        st.markdown("#### ğŸ“ˆ ë¯¼ê°ë„ ë¶„ì„ (í† ë„¤ì´ë„)")
        base_sale = st.number_input("ê¸°ì¤€ ë¶„ì–‘ê°€(ë§Œì›/ã¡)", 500.0, 3000.0, 1200.0, 10.0)
        base_cost = st.number_input("ê¸°ì¤€ ê³µì‚¬ë¹„(ë§Œì›/ã¡)", 300.0, 2500.0, 900.0, 10.0)
        base_bus  = st.slider("ê¸°ì¤€ ë²„ìŠ¤ì¦í¸(%)", 0, 100, 20, 5)
        base_infra= st.number_input("ê¸°ì¤€ ì¸í”„ë¼(ì–µì›)", 0.0, 1000.0, 30.0, 5.0)

        # Â±ë³€ë™í­
        pct = st.slider("ë³€ë™í­(Â±%)", 1, 30, 15, 1)

        def kpi_with(sale, cost, bus, infra):
            return calc_kpis(households, avg_py, sale, cost, infra, congestion_base, bus,
                             non_sale_ratio, sale_rate, disc_rate, years)["NPV(ì–µì›)"]

        base_npv = kpi_with(base_sale, base_cost, base_bus, base_infra)
        factors = []
        for name, (lo, hi) in {
            "ë¶„ì–‘ê°€": (base_sale*(1-pct/100), base_sale*(1+pct/100)),
            "ê³µì‚¬ë¹„": (base_cost*(1-pct/100), base_cost*(1+pct/100)),
            "ë²„ìŠ¤ì¦í¸": (max(0, base_bus-pct), min(100, base_bus+pct)),
            "ì¸í”„ë¼": (max(0, base_infra*(1-pct/100)), base_infra*(1+pct/100)),
        }.items():
            npv_lo = kpi_with(lo if name=="ë¶„ì–‘ê°€" else base_sale,
                              lo if name=="ê³µì‚¬ë¹„" else base_cost,
                              lo if name=="ë²„ìŠ¤ì¦í¸" else base_bus,
                              lo if name=="ì¸í”„ë¼" else base_infra)
            npv_hi = kpi_with(hi if name=="ë¶„ì–‘ê°€" else base_sale,
                              hi if name=="ê³µì‚¬ë¹„" else base_cost,
                              hi if name=="ë²„ìŠ¤ì¦í¸" else base_bus,
                              hi if name=="ì¸í”„ë¼" else base_infra)
            factors.append({"ìš”ì¸": name, "NPV_low": npv_lo, "NPV_high": npv_hi})

        df_tornado = pd.DataFrame(factors)
        bars = alt.Chart(df_tornado).transform_fold(
            ["NPV_low","NPV_high"], as_=["type","NPV"]
        ).mark_bar().encode(
            y=alt.Y("ìš”ì¸:N", sort=None),
            x=alt.X("NPV:Q", title="NPV(ì–µì›)"),
            color="type:N",
            tooltip=["ìš”ì¸:N","NPV:Q"]
        ).properties(height=200)
        st.altair_chart(bars, use_container_width=True)

    # ---------------------------
    # 3) í™•ë¥ (ê°„ì´) Monte Carlo
    # ---------------------------
    with tab3:
        st.markdown("#### ğŸ² í™•ë¥  ë¶„ì„ (ê°„ì´ Monte Carlo)")
        n = st.slider("ì‹œë®¬ë ˆì´ì…˜ ë°˜ë³µìˆ˜", 200, 5000, 1000, 100)
        sigma_sale = st.slider("ë¶„ì–‘ê°€ í‘œì¤€í¸ì°¨(%)", 1, 20, 7)
        sigma_cost = st.slider("ê³µì‚¬ë¹„ í‘œì¤€í¸ì°¨(%)", 1, 20, 5)

        rng = np.random.default_rng(42)
        sale_samples = rng.normal(loc=base_sale, scale=base_sale*sigma_sale/100, size=n)
        cost_samples = rng.normal(loc=base_cost, scale=base_cost*sigma_cost/100, size=n)

        def kpi_with(sale, cost, bus, infra):
            return calc_kpis(households, avg_py, sale, cost, infra, congestion_base, bus,
                             non_sale_ratio, sale_rate, disc_rate, years)["NPV(ì–µì›)"]

        npvs = []
        for s, c in zip(sale_samples, cost_samples):
            npvs.append(kpi_with(max(100, s), max(100, c), base_bus, base_infra))
        ser = pd.Series(npvs)
        p10, p50, p90 = np.percentile(ser, [10,50,90])

        st.metric("P10 NPV", f"{p10:,.1f} ì–µì›")
        st.metric("P50 NPV", f"{p50:,.1f} ì–µì›")
        st.metric("P90 NPV", f"{p90:,.1f} ì–µì›")
        st.caption("â€» P10: ë³´ìˆ˜ì (í•˜ìœ„ 10%), P90: ë‚™ê´€ì (ìƒìœ„ 10%)")

        hist = alt.Chart(pd.DataFrame({"NPV": ser})).mark_bar().encode(
            x=alt.X("NPV:Q", bin=alt.Bin(maxbins=30), title="NPV(ì–µì›)"),
            y="count()"
        ).properties(height=200)
        st.altair_chart(hist, use_container_width=True)

    # ---------------------------
    # 4) ë¦¬í¬íŠ¸/ì²´í¬ë¦¬ìŠ¤íŠ¸
    # ---------------------------
    with tab4:
        st.markdown("#### ğŸ§· í–‰ì • í˜‘ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ìë™ ìƒì„±)")
        imp = float(df_scn.loc["A","í˜¼ì¡ë„ê°œì„ (Î”%)"]) if "A" in df_scn.index else 0.0
        msg = []
        if imp >= 5:
            msg.append("â€¢ êµí†µì˜í–¥í‰ê°€ í˜‘ì˜ ì‹œ, **í˜¼ì¡ë„ ê°œì„  Î”â‰¥5%** ê·¼ê±° ì œì‹œ (ë²„ìŠ¤ ì¦í¸ + ë…¸ì„  ìµœì í™”)")
        else:
            msg.append("â€¢ í˜¼ì¡ë„ ê°œì„ ì´ ì‘ìŒ â†’ **ì •ë¥˜ì¥ ìœ„ì¹˜/í™˜ìŠ¹í¸ì˜** ì‹œë®¬ë ˆì´ì…˜ ë³´ì™„ ê¶Œê³ ")
        if df_scn["ë§ˆì§„ìœ¨(%)"].max() < 10:
            msg.append("â€¢ ë§ˆì§„ìœ¨ ë‚®ìŒ â†’ ê³µì‚¬ë¹„ ë‹¨ê°€/í‰í˜• ë¯¹ìŠ¤/ë¹„ë¶„ì–‘ ë¹„ìœ¨ ì¬ê²€í†  ê¶Œê³ ")
        if df_scn["NPV(ì–µì›)"].max() < 0:
            msg.append("â€¢ NPV ìŒìˆ˜ â†’ ë¶„ì–‘ê°€ ì‚°ì • ì¬ê²€í†  ë˜ëŠ” ì¸í”„ë¼ íˆ¬ì ì¶•ì†Œ í•„ìš”")
        st.write("\n".join(msg))

        st.markdown("#### ğŸ“¤ ë‚´ë³´ë‚´ê¸°")
        export_df = df_scn.reset_index()
        st.download_button("â¬‡ï¸ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµí‘œ(CSV)", data=export_df.to_csv(index=False).encode("utf-8-sig"),
                           file_name="scenario_compare.csv", mime="text/csv")

        # ê°„ë‹¨ PDF (í…ìŠ¤íŠ¸ ìœ„ì£¼)
        from reportlab.lib.pagesizes import A4
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
        from reportlab.lib.styles import getSampleStyleSheet
        from reportlab.lib import colors

        def export_pdf_simple():
            path = "AIoT_Biz_Analysis_Report.pdf"
            styles = getSampleStyleSheet()
            doc = SimpleDocTemplate(path, pagesize=A4)
            story = [
                Paragraph("AIoT ìŠ¤ë§ˆíŠ¸ ì¸í”„ë¼ ëŒ€ì‹œë³´ë“œ â€” ì‚¬ì—…ì„± ë¶„ì„ ë¦¬í¬íŠ¸", styles["Title"]),
                Spacer(1, 12),
                Paragraph(f"ëŒ€ìƒ: {current.get('address_display','')}", styles["Normal"]),
                Paragraph(f"ìì¹˜êµ¬: {current.get('gu','')}", styles["Normal"]),
                Spacer(1, 12),
                Paragraph("ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ ìš”ì•½", styles["Heading2"]),
            ]
            # í‘œ ë°ì´í„°
            tbl_data = [export_df.columns.tolist()] + export_df.astype(str).values.tolist()
            tbl = Table(tbl_data, repeatRows=1)
            tbl.setStyle(TableStyle([
                ("BACKGROUND", (0,0), (-1,0), colors.black),
                ("TEXTCOLOR", (0,0), (-1,0), colors.whitesmoke),
                ("GRID", (0,0), (-1,-1), 0.25, colors.grey),
                ("ALIGN", (0,0), (-1,-1), "CENTER"),
            ]))
            story += [tbl, Spacer(1, 12)]
            story += [Paragraph("í–‰ì • í˜‘ì˜ ì²´í¬í¬ì¸íŠ¸", styles["Heading2"])]
            for t in msg:
                story += [Paragraph(t, styles["Normal"])]
            story += [Spacer(1, 12), Paragraph("ê°€ì •/íŒŒë¼ë¯¸í„° ë¡œê·¸", styles["Heading2"])]
            story += [Paragraph(f"ì„¸ëŒ€ìˆ˜={households:,}, í‰ê· ì „ìš©={avg_py}í‰, ë¹„ë¶„ì–‘={int(non_sale_ratio*100)}%, ë¶„ì–‘ë¥ ={int(sale_rate*100)}%", styles["Normal"])]
            story += [Paragraph(f"í• ì¸ìœ¨={disc_rate*100:.1f}%, íšŒìˆ˜ê¸°ê°„={years}ë…„, ê¸°ì¤€í˜¼ì¡ë„={congestion_base}%", styles["Normal"])]
            doc.build(story)
            return path

        if st.button("ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ"):
            pdf_path = export_pdf_simple()
            st.success(f"PDF ìƒì„± ì™„ë£Œ: {pdf_path}")
