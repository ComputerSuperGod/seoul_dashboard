# -------------------------------------------------------------
# ğŸ— AIoT ìŠ¤ë§ˆíŠ¸ ì¸í”„ë¼ ëŒ€ì‹œë³´ë“œ (ì¬ê±´ì¶• ì˜ì‚¬ê²°ì • Helper)
# -------------------------------------------------------------
# ğŸ“Š CSV ê¸°ë°˜ ë°ì´í„° ë°˜ì˜ ë²„ì „
# -------------------------------------------------------------

# --- must come first: add project root to sys.path BEFORE importing utils ---
import sys
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent  # .../seoul_dashboard_3
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))
# ---------------------------------------------------------------------------

import streamlit as st
import pandas as pd
import numpy as np
import pydeck as pdk
import altair as alt
import geopandas as gpd

from components.sidebar_faq import render_sidebar_faq
import streamlit as st



from datetime import datetime

# === [SESSION KEYS INIT] ===
if "matched_links_geojson" not in st.session_state:
    st.session_state["matched_links_geojson"] = None
if "matched_links_geojson_daily" not in st.session_state:
    st.session_state["matched_links_geojson_daily"] = None

# === ì™¸ë¶€ ëª¨ë“ˆ (utils) ì„í¬íŠ¸ ===
from utils.traffic_preproc import ensure_speed_csv

# Altair/MPL/Plotly ìŠ¤ìœ„ì¹˜í˜•: plot_speedê°€ ì—†ê±°ë‚˜ ë¡œë”© ì‹¤íŒ¨í•˜ë©´ ê¸°ì¡´ í•¨ìˆ˜ë¡œ í´ë°±
try:
    from utils.traffic_plot import plot_speed
    _HAS_PLOT_SPEED = True
except Exception as e:
    print("utils.traffic_plot import fallback:", e)
    from utils.traffic_plot import plot_nearby_speed_from_csv
    _HAS_PLOT_SPEED = False

# === ë°ì´í„° ë””ë ‰í„°ë¦¬ ===
DATA_DIR = BASE_DIR / "data"

# === ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ì´ë¦„ ì¶©ëŒ ë°©ì§€: ë³€ìˆ˜ëª… êµ¬ë¶„) ===
BASE_YEAR = 2023   # 24ë…„ìœ¼ë¡œ ë°”ê¿”ë„ ë¨

# ì¬ê±´ì¶• í”„ë¡œì íŠ¸ ì›ë³¸ CSV (ë‹¹ì‹ ì˜ appì—ì„œ ì“°ë˜ í‘œ ë°ì´í„°)
PROJECTS_CSV_PATH = DATA_DIR / "seoul_redev_projects.csv"

# êµí†µ ê¸°ì¤€ë…„ë„ ë°ì´í„° (ì—‘ì…€ â†’ CSV ìë™ ë³€í™˜ ëŒ€ìƒ)
TRAFFIC_XLSX_PATH = DATA_DIR / "AverageSpeed(LINK).xlsx"
TRAFFIC_CSV_PATH  = DATA_DIR / f"AverageSpeed_Seoul_{BASE_YEAR}.csv"

# ë„ë¡œë§ ë ˆë²¨55 ì‰ì´í”„
SHP_PATH = DATA_DIR / "seoul_link_lev5.5_2023.shp"
LINK_ID_COL = "k_link_id"  # âœ… SHPì˜ ë§í¬ ì»¬ëŸ¼ëª…

#===========================ìºì‹œ ë¹„ìš°ê¸°=================
with st.sidebar:
    if st.button("ìºì‹œ ë¹„ìš°ê¸°"):
        st.cache_data.clear()
        st.session_state["matched_links_geojson"] = None
        st.session_state["matched_links_geojson_daily"] = None
        st.rerun()
#======================================================

# ğŸ”¤ ì•ˆì „í•œ CSV ë¡œë”: ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
def smart_read_csv(path, encodings=("utf-8-sig", "cp949", "euc-kr", "utf-8", "latin1")):
    import pandas as pd
    last_err = None
    for enc in encodings:
        try:
            df = pd.read_csv(path, encoding=enc)
            return df
        except UnicodeDecodeError as e:
            last_err = e
            continue
    # ìµœí›„ ìˆ˜ë‹¨: errors='replace' ë¡œë¼ë„ ì½ê¸°
    try:
        df = pd.read_csv(path, encoding="utf-8", errors="replace")
        return df
    except Exception:
        raise last_err or Exception(f"Failed to read {path} with tried encodings.")

# -------------------------------------------------------------
# âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ ìë™ ì„¤ì •
# -------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
CSV_PATH = BASE_DIR / "data" / "seoul_redev_projects.csv"
CSV_ENCODING = "cp949"

# -------------------------------------------------------------
# ğŸ“¦ ì¢Œí‘œ CSV ë³‘í•© ìœ í‹¸
# -------------------------------------------------------------
COORD_CSV_PATH = BASE_DIR / "data" / "ì„œìš¸ì‹œ_ì¬ê°œë°œì¬ê±´ì¶•_clean_kakao.csv"
COORD_ENCODING = "utf-8-sig"  # (ìŠ¤ë§ˆíŠ¸ ë¡œë”ê°€ ìë™íŒë³„í•˜ë¯€ë¡œ ì—†ì–´ë„ ë™ì‘)

# 1) CSV ë¡œë“œ ìœ í‹¸ (êµí†µëŸ‰ CSV)
@st.cache_data(show_spinner=False)
def load_volume_csv(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    df["link_id"] = df["link_id"].astype(str)

    # ì‹œê°„ ì•ˆì „ ì •ê·œí™”: "0ì‹œ", " 08 ", "8.0" ë“±ë„ 0~23ìœ¼ë¡œ ë³€í™˜
    h = pd.to_numeric(
        pd.Series(df["hour"]).astype(str).str.extract(r"(\d+)", expand=False),
        errors="coerce"
    ).fillna(0).astype(int) % 24
    df["hour"] = h

    df["ì°¨ëŸ‰ëŒ€ìˆ˜"] = pd.to_numeric(df["ì°¨ëŸ‰ëŒ€ìˆ˜"], errors="coerce").fillna(0)
    return df

# 2) êµí†µëŸ‰ ê°€ì¤‘ í˜¼ì¡ë¹ˆë„ê°•ë„(CFI) ê³„ì‚°
def compute_cfi_weighted(speed_df: pd.DataFrame, vol_df: pd.DataFrame, boundary_speed: float = 30.0):
    d = speed_df.copy()
    d["link_id"] = d["link_id"].astype(str)
    d["hour"] = d["hour"].astype(int)
    d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")

    # merge
    m = d.merge(vol_df, on=["link_id", "hour"], how="inner")

    # í˜¼ì¡ ì°¨ëŸ‰ìˆ˜(ì†ë„<=ê²½ê³„ê°’) vs ì „ì²´ ì°¨ëŸ‰ìˆ˜
    m["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] = (m["í‰ê· ì†ë„(km/h)"] <= boundary_speed).astype(int) * m["ì°¨ëŸ‰ëŒ€ìˆ˜"]
    g = m.groupby(["link_id", "hour"], as_index=False).agg(
        ì „ì²´ì°¨ëŸ‰ìˆ˜=("ì°¨ëŸ‰ëŒ€ìˆ˜", "sum"),
        í˜¼ì¡ì°¨ëŸ‰ìˆ˜=("í˜¼ì¡ì°¨ëŸ‰ìˆ˜", "sum"),
    )
    g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = (g["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] / g["ì „ì²´ì°¨ëŸ‰ìˆ˜"]).replace([float("inf"), float("nan")], 0) * 100
    return g

def compute_cfi_soft(
    speed_df: pd.DataFrame,
    vol_df: pd.DataFrame,
    boundary_mode: str = "percentile",  # "percentile" or "fixed"
    boundary_value: float = 40.0,       # percentile: 10~90(%), fixed: km/h
    tau_kmh: float = 6.0                # ì‹œê·¸ëª¨ì´ë“œ ê¸‰ê²½ì‚¬ í­(ê°’ì´ í¬ë©´ ë” ë¶€ë“œëŸ¬ì›€)
):
    """
    í‰ê· ì†ë„(ì‹œê°„ëŒ€ë³„ 1ê°œ) + ì‹œê°„ëŒ€ ì´ ì°¨ëŸ‰ëŒ€ìˆ˜ë§Œ ìˆì„ ë•Œ
    ì‹œê·¸ëª¨ì´ë“œ ê¸°ë°˜ì˜ 'ë¶€ë“œëŸ¬ìš´' í˜¼ì¡ í™•ë¥ ì„ ë§Œë“¤ì–´ êµí†µëŸ‰ ê°€ì¤‘ CFI ê·¼ì‚¬.
    """
    # --- ì†ë„ ë°ì´í„° ì •ë¦¬ ---
    d = speed_df.copy()
    d["link_id"] = d["link_id"].astype(str)
    d["hour"] = pd.to_numeric(d["hour"], errors="coerce").astype("Int64")  # allow NA
    d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")

    # --- êµí†µëŸ‰ ë°ì´í„° ì •ë¦¬ ---
    v = vol_df.copy()
    v["link_id"] = v["link_id"].astype(str)
    v["hour"] = pd.to_numeric(v["hour"], errors="coerce").astype("Int64") % 24
    v["ì°¨ëŸ‰ëŒ€ìˆ˜"] = pd.to_numeric(v["ì°¨ëŸ‰ëŒ€ìˆ˜"], errors="coerce").fillna(0)

    # --- ë³‘í•© ---
    m = d.merge(v, on=["link_id", "hour"], how="inner").dropna(subset=["í‰ê· ì†ë„(km/h)"])
    if m.empty:
        out = m[["link_id","hour"]].copy()
        out["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = 0.0
        out.attrs = {"boundary": np.nan, "mode": boundary_mode}
        return out

    # --- ê²½ê³„ì†ë„ ê²°ì • ---
    if boundary_mode == "percentile":
        p = float(boundary_value)
        p = max(5.0, min(95.0, p))  # ì•ˆì „ ë²”ìœ„
        vb = float(np.nanpercentile(m["í‰ê· ì†ë„(km/h)"], p))
    else:
        vb = float(boundary_value)

    # --- ì‹œê·¸ëª¨ì´ë“œ í˜¼ì¡í™•ë¥  ---
    tau = max(1e-6, float(tau_kmh))
    m["p_cong"] = 1.0 / (1.0 + np.exp((m["í‰ê· ì†ë„(km/h)"] - vb) / tau))

    # --- ë§í¬Ã—ì‹œê°„ëŒ€ë¡œ ì§‘ê³„ (êµí†µëŸ‰ ê°€ì¤‘ í‰ê· ) ---
    def _wavg(x, w):
        w = np.asarray(w)
        x = np.asarray(x)
        mask = np.isfinite(x) & np.isfinite(w) & (w >= 0)
        if not mask.any():
            return 0.0
        return float((x[mask] * w[mask]).sum() / max(1e-9, w[mask].sum()))

    g = (
        m.groupby(["link_id","hour"], as_index=False)
         .apply(lambda df: pd.Series({
             "í˜¼ì¡ë¹ˆë„ê°•ë„(%)": _wavg(df["p_cong"], df["ì°¨ëŸ‰ëŒ€ìˆ˜"]) * 100.0
         }))
         .reset_index()
    )

    # ì•ˆì „ í´ë¦½
    g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"].clip(0, 100)
    g.attrs = {"boundary": vb, "mode": boundary_mode, "tau": tau}
    return g

@st.cache_data(show_spinner=False)
def load_coords() -> pd.DataFrame:
    df = smart_read_csv(COORD_CSV_PATH)

    # ìˆ«ìí™”
    df["lat"] = pd.to_numeric(df.get("lat"), errors="coerce")
    df["lon"] = pd.to_numeric(df.get("lon"), errors="coerce")

    # ì•ˆì „í•œ ë³‘í•©ìš© ê¸°ì´ˆ ì»¬ëŸ¼ ìƒì„±
    def coalesce(a, b):
        a = "" if a is None else str(a).strip()
        b = "" if b is None else str(b).strip()
        return a if a else b

    # name, address ë§Œë“¤ê¸°
    if ("ì •ë¹„êµ¬ì—­ëª…ì¹­" in df.columns) or ("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…" in df.columns):
        name = [coalesce(n, m) for n, m in zip(df.get("ì •ë¹„êµ¬ì—­ëª…ì¹­"), df.get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"))]
    else:
        name = df.get("name")

    address = [coalesce(a, b) for a, b in zip(df.get("ì •ë¹„êµ¬ì—­ìœ„ì¹˜"), df.get("ëŒ€í‘œì§€ë²ˆ"))]

    out = pd.DataFrame({
        "apt_id": df.get("ì‚¬ì—…ë²ˆí˜¸").astype(str) if "ì‚¬ì—…ë²ˆí˜¸" in df.columns else "",
        "name": pd.Series(name, index=df.index),
        "gu": df.get("ìì¹˜êµ¬"),
        "address": pd.Series(address, index=df.index),
        "full_address": df.get("full_address"),
        "lat": df["lat"],
        "lon": df["lon"],
    })
    for col in ["apt_id", "name", "gu", "address", "full_address"]:
        if col in out.columns:
            out[col] = out[col].fillna("").astype(str).str.strip()
    return out

@st.cache_data(show_spinner=False)
def merge_projects_with_coords(gu: str) -> pd.DataFrame:
    # 1) ì›ë³¸ ë¡œë“œ + ìŠ¤í‚¤ë§ˆ í†µì¼
    raw = load_raw_csv()
    proj = normalize_schema(raw)
    proj = proj[proj["gu"] == gu].copy()

    # 2) ì¢Œí‘œ ë¡œë“œ
    coords = load_coords()
    coords = coords[coords["gu"] == gu].copy()

    # 3) âœ… name+gu ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­í•˜ê³  full_addressê¹Œì§€ ê°€ì ¸ì˜¤ê¸°
    out = proj.merge(
        coords[["name", "gu", "lat", "lon", "full_address"]],
        on=["name", "gu"],
        how="left"
    )

    # 4) ì¢Œí‘œ ê²°ì¸¡ ë³´ì • (êµ¬ ì¤‘ì‹¬ + ì§€í„°)
    missing = out["lat"].isna() | out["lon"].isna()
    base_lat, base_lon = GU_CENTER.get(gu, (37.55, 127.0))
    rng = np.random.default_rng(42)
    jitter = lambda n: rng.normal(0, 0.002, n)  # â‰ˆ 200m
    if missing.any():
        n = int(missing.sum())
        out.loc[missing, "lat"] = base_lat + jitter(n)
        out.loc[missing, "lon"] = base_lon + jitter(n)
    out["has_geo"] = ~missing

    # 5) âœ… í‘œì‹œìš© ì£¼ì†Œ: full_address ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸ address
    out["address_display"] = out["full_address"].fillna("").replace("", pd.NA)
    out["address_display"] = out["address_display"].fillna(out["address"]).fillna("")

    return out.reset_index(drop=True)

# -------------------------------------------------------------
# âš™ï¸ Streamlit ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------------------
st.set_page_config(
    page_title="AIoT ìŠ¤ë§ˆíŠ¸ ì¸í”„ë¼ ëŒ€ì‹œë³´ë“œ | ì¬ê±´ì¶• Helper",
    layout="wide",
)

st.markdown("""
<style>
/* ëª¨ë“  LaTeX ìˆ˜ì‹ì„ ì™¼ìª½ ì •ë ¬ë¡œ */
.katex-display {
    text-align: left !important;
    margin-left: 0 !important;
}

/* í…ìŠ¤íŠ¸ ì „ì²´ ê¸°ë³¸ ì™¼ìª½ ì •ë ¬ ìœ ì§€ */
.block-container {
    text-align: left !important;
}
</style>
""", unsafe_allow_html=True)

STYLE = """
<style>
.small-muted { color: #7a7a7a; font-size: 0.9rem; }
.kpi-card { padding: 16px; border-radius: 16px; background: #111827; border: 1px solid #1f2937; }
.kpi-title { font-size: 0.9rem; color: #9ca3af; margin-bottom: 8px; }
.kpi-value { font-size: 1.4rem; font-weight: 700; color: #e5e7eb; }
.section-title { font-size: 1.1rem; font-weight: 700; margin-bottom: 8px; }
hr.soft { border: none; height: 1px; background: #1f2937; margin: 16px 0; }
</style>
"""
st.markdown(STYLE, unsafe_allow_html=True)

st.markdown("""
<style>
div[data-testid="stMarkdownContainer"] .katex-display {
    text-align: left !important;
    margin-left: 0 !important;
    margin-right: auto !important;
}
div[data-testid="stMarkdownContainer"] .katex-display > .katex {
    display: inline-block !important;
}
div[data-testid="stMarkdownContainer"] p {
    text-align: left !important;
    margin-left: 0 !important;
}
</style>
""", unsafe_allow_html=True)

# -------------------------------------------------------------
# ğŸ§¾ CSV ë¡œë“œ & ì „ì²˜ë¦¬
# -------------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_raw_csv() -> pd.DataFrame:
    return smart_read_csv(PROJECTS_CSV_PATH)  # âœ… ì¸ì½”ë”© ìë™ ê°ì§€ ì‚¬ìš©

def _coalesce(*vals):
    for v in vals:
        if pd.notna(v) and str(v).strip():
            return v
    return None

def normalize_schema(df_raw: pd.DataFrame) -> pd.DataFrame:
    c = df_raw.columns
    get = lambda name: df_raw[name] if name in c else pd.Series([None]*len(df_raw))

    def _num(x):
        return pd.to_numeric(pd.Series(x, index=df_raw.index), errors="coerce")

    def _pct_to_num(x):
        s = pd.Series(x, index=df_raw.index).astype(str).str.replace("%", "", regex=False)
        s = s.str.replace(",", "", regex=False)
        return pd.to_numeric(s, errors="coerce")

    def _floors_to_num(x):
        s = pd.Series(x, index=df_raw.index).astype(str).str.extract(r"(-?\d+)", expand=False)
        return pd.to_numeric(s, errors="coerce")

    # ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ë§¤í•‘
    df = pd.DataFrame({
        "apt_id": get("ì‚¬ì—…ë²ˆí˜¸"),
        "name": [v if (pd.notna(v) and str(v).strip()) else w for v, w in zip(get("ì •ë¹„êµ¬ì—­ëª…ì¹­"), get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"))],
        "org_name": get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"),
        "biz_type": get("ì‚¬ì—…êµ¬ë¶„"),
        "op_type": get("ìš´ì˜êµ¬ë¶„"),
        "gu": get("ìì¹˜êµ¬"),
        "address": [v if (pd.notna(v) and str(v).strip()) else w for v, w in zip(get("ì •ë¹„êµ¬ì—­ìœ„ì¹˜"), get("ëŒ€í‘œì§€ë²ˆ"))],
        "households": _num(get("ë¶„ì–‘ì„¸ëŒ€ì´ìˆ˜")),
        "land_area_m2": _num(get("ì •ë¹„êµ¬ì—­ë©´ì (ã¡)")),
        "far": _pct_to_num(get("ìš©ì ë¥ ")),
        "floors": _floors_to_num(get("ì¸µìˆ˜")),
        "status": get("ì§„í–‰ë‹¨ê³„"),
        "floors_up": _floors_to_num(get("ì§€ìƒì¸µìˆ˜")),
        "floors_down": _floors_to_num(get("ì§€í•˜ì¸µìˆ˜")),
    })

    def _fmt_floor(u, d):
        parts = []
        if pd.notna(u):
            parts.append(f"ì§€ìƒ {int(u)}")
        if pd.notna(d):
            parts.append(f"ì§€í•˜ {int(d)}")
        return " / ".join(parts)

    df["floors_display"] = [_fmt_floor(u, d) for u, d in zip(df["floors_up"], df["floors_down"])]
    df["floors_show"] = df["floors_display"].fillna("").astype(str).str.strip()
    mask_empty = df["floors_show"] == ""
    df.loc[mask_empty, "floors_show"] = df.loc[mask_empty, "floors"].apply(
        lambda x: f"{int(x)}ì¸µ" if pd.notna(x) else ""
    )

    df["apt_id"] = df["apt_id"].astype(str)
    df["name"] = df["name"].fillna("ë¬´ëª… ì •ë¹„êµ¬ì—­")
    for col in ["org_name","biz_type","op_type","gu","address","status"]:
        df[col] = df[col].fillna("").astype(str).str.strip()

    return df

@st.cache_data(show_spinner=False)
def get_projects_by_gu(gu: str) -> pd.DataFrame:
    raw = load_raw_csv()
    norm = normalize_schema(raw)
    return norm[norm["gu"] == gu].reset_index(drop=True)

# -------------------------------------------------------------
# ğŸ“ ì„œìš¸ì‹œ 25ê°œ ìì¹˜êµ¬ ë¦¬ìŠ¤íŠ¸ & ì¤‘ì‹¬ì¢Œí‘œ
# -------------------------------------------------------------
DISTRICTS = [
    "ê°•ë‚¨êµ¬","ê°•ë™êµ¬","ê°•ë¶êµ¬","ê°•ì„œêµ¬","ê´€ì•…êµ¬","ê´‘ì§„êµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬",
    "ë…¸ì›êµ¬","ë„ë´‰êµ¬","ë™ëŒ€ë¬¸êµ¬","ë™ì‘êµ¬","ë§ˆí¬êµ¬","ì„œëŒ€ë¬¸êµ¬","ì„œì´ˆêµ¬","ì„±ë™êµ¬",
    "ì„±ë¶êµ¬","ì†¡íŒŒêµ¬","ì–‘ì²œêµ¬","ì˜ë“±í¬êµ¬","ìš©ì‚°êµ¬","ì€í‰êµ¬","ì¢…ë¡œêµ¬","ì¤‘êµ¬","ì¤‘ë‘êµ¬"
]

GU_CENTER = {
    "ê°•ë‚¨êµ¬": (37.5172, 127.0473),
    "ê°•ë™êµ¬": (37.5301, 127.1238),
    "ê°•ë¶êµ¬": (37.6396, 127.0257),
    "ê°•ì„œêµ¬": (37.5509, 126.8495),
    "ê´€ì•…êµ¬": (37.4784, 126.9516),
    "ê´‘ì§„êµ¬": (37.5386, 127.0822),
    "êµ¬ë¡œêµ¬": (37.4955, 126.8876),
    "ê¸ˆì²œêµ¬": (37.4569, 126.8958),
    "ë…¸ì›êµ¬": (37.6543, 127.0565),
    "ë„ë´‰êµ¬": (37.6688, 127.0471),
    "ë™ëŒ€ë¬¸êµ¬": (37.5740, 127.0396),
    "ë™ì‘êµ¬": (37.5124, 126.9393),
    "ë§ˆí¬êµ¬": (37.5638, 126.9084),
    "ì„œëŒ€ë¬¸êµ¬": (37.5791, 126.9368),
    "ì„œì´ˆêµ¬": (37.4836, 127.0326),
    "ì„±ë™êµ¬": (37.5633, 127.0369),
    "ì„±ë¶êµ¬": (37.5894, 127.0167),
    "ì†¡íŒŒêµ¬": (37.5145, 127.1068),
    "ì–‘ì²œêµ¬": (37.5169, 126.8665),
    "ì˜ë“±í¬êµ¬": (37.5264, 126.8963),
    "ìš©ì‚°êµ¬": (37.5311, 126.9811),
    "ì€í‰êµ¬": (37.6176, 126.9227),
    "ì¢…ë¡œêµ¬": (37.5736, 126.9780),
    "ì¤‘êµ¬": (37.5636, 126.9976),
    "ì¤‘ë‘êµ¬": (37.6063, 127.0929),
}

def attach_latlon_by_gu_centroid(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["lat"] = df["gu"].map(lambda g: GU_CENTER.get(g, (37.55, 127.0))[0]) + np.random.normal(scale=0.004, size=len(df))
    df["lon"] = df["gu"].map(lambda g: GU_CENTER.get(g, (37.55, 127.0))[1]) + np.random.normal(scale=0.004, size=len(df))
    return df

# -------------------------------------------------------------
# ğŸ§­ ì‚¬ì´ë“œë°”
# -------------------------------------------------------------
st.sidebar.title("ì¬ê±´ì¶• ì˜ì‚¬ê²°ì • Helper")

# ì‚¬ì´ë“œë°” - êµ¬ ì„ íƒ
selected_gu = st.sidebar.selectbox("êµ¬ ì„ íƒ", DISTRICTS, index=0)

# âœ… ì„¸ì…˜ì— ì„ íƒ ì¸ë±ìŠ¤ ì´ˆê¸°í™”(ë§¨ ìœ„ í•œ ë²ˆ)
if "selected_row" not in st.session_state:
    st.session_state.selected_row = None
if "selected_gu_prev" not in st.session_state:
    st.session_state.selected_gu_prev = selected_gu

if st.session_state.selected_gu_prev != selected_gu:
    st.session_state.selected_row = None
    st.session_state.selected_gu_prev = selected_gu

st.sidebar.markdown(
    "<div class='small-muted'>êµ¬ ì„ íƒ ì‹œ, í•´ë‹¹ êµ¬ì˜ ì •ë¹„ì‚¬ì—… ë‹¨ì§€ ëª©ë¡ê³¼ ì§€ë„ê°€ ê°±ì‹ ë©ë‹ˆë‹¤.</div>",
    unsafe_allow_html=True,
)

project_name = st.sidebar.text_input("í”„ë¡œì íŠ¸ ì´ë¦„", value=f"{selected_gu} ì¬ê±´ì¶• ì‹œë‚˜ë¦¬ì˜¤")

# ì‚¬ì´ë“œë°” FAQ ë Œë”
render_sidebar_faq()


# -------------------------------------------------------------
# ğŸ—ºï¸ 1-2ì‚¬ë¶„ë©´: ì§€ë„ + ë‹¨ì§€ ì„ íƒ
# -------------------------------------------------------------
col12_left, col12_right = st.columns([2.2, 1.4], gap="large")

with col12_left:
    st.markdown("### ğŸ—º 1-2ì‚¬ë¶„ë©´ Â· ì§€ë„ & ë‹¨ì§€ì„ íƒ")

    base_df = get_projects_by_gu(selected_gu)
    df_map = merge_projects_with_coords(selected_gu)

    # ì§€ë„ ìë¦¬ë§Œ ë¨¼ì € í™•ë³´ (í•„í„°/ì„ íƒ ì ìš© í›„ ì•„ë˜ì—ì„œ ì‹¤ì œ ì°¨íŠ¸ ë Œë”)
    map_slot = st.empty()

    if df_map.empty:
        st.warning("âš ï¸ í•´ë‹¹ êµ¬ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

# ---------------------------
# ğŸ“‹ ë‹¨ì§€ í…Œì´ë¸” + í•„í„° UI
# ---------------------------
with st.expander("ğŸ” ë°ì´í„° ì†ŒìŠ¤ í™•ì¸(ì„ì‹œ)", expanded=False):
    raw = load_raw_csv()
    cols_raw = ["ìì¹˜êµ¬", "ì •ë¹„êµ¬ì—­ëª…ì¹­", "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…", "ë¶„ì–‘ì„¸ëŒ€ì´ìˆ˜", "ì •ë¹„êµ¬ì—­ë©´ì (ã¡)"]
    exist_cols = [c for c in cols_raw if c in raw.columns]
    st.caption(f"ì›ë³¸: {CSV_PATH.name} Â· í‘œì‹œì—´: {', '.join(exist_cols)}")
    try:
        st.dataframe(
            raw[exist_cols][raw["ìì¹˜êµ¬"] == selected_gu].head(20),
            use_container_width=True
        )
    except Exception:
        st.dataframe(raw[exist_cols].head(20), use_container_width=True)

    st.caption("í˜„ì¬ í‘œì‹œê°’(df_map)")
    st.dataframe(
        df_map[["gu", "address_display", "name", "households", "land_area_m2"]].head(20),
        use_container_width=True
    )

    def _coalesce(a, b):
        a = "" if a is None else str(a).strip()
        b = "" if b is None else str(b).strip()
        return a if a else b

    raw_norm = pd.DataFrame({
        "gu": raw.get("ìì¹˜êµ¬"),
        "name_raw": [
            _coalesce(n, m) for n, m in zip(
                raw.get("ì •ë¹„êµ¬ì—­ëª…ì¹­"),
                raw.get("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…")
            )
        ],
        "households_src": pd.to_numeric(raw.get("ë¶„ì–‘ì„¸ëŒ€ì´ìˆ˜"), errors="coerce"),
        "land_area_m2_src": pd.to_numeric(raw.get("ì •ë¹„êµ¬ì—­ë©´ì (ã¡)"), errors="coerce"),
    })

    comp = df_map.merge(
        raw_norm, left_on=["name","gu"], right_on=["name_raw","gu"], how="left"
    )

    hh_match  = (comp["households"].fillna(-1).astype(float) == comp["households_src"].fillna(-1).astype(float))
    area_match = np.isclose(
        comp["land_area_m2"].astype(float),
        comp["land_area_m2_src"].astype(float),
        rtol=1e-6, atol=1e-6, equal_nan=True
    )

    comp["households_match"] = hh_match
    comp["land_area_match"]  = area_match

    total = len(comp)
    hh_ok  = int(hh_match.sum())
    ar_ok  = int(area_match.sum())

    st.write(
        f"âœ… ì„¸ëŒ€ìˆ˜ ì¼ì¹˜: **{hh_ok}/{total}**  Â·  "
        f"âœ… ë©´ì  ì¼ì¹˜: **{ar_ok}/{total}**"
    )

    bad = comp[~(hh_match & area_match)][
        ["gu","address_display","name","households","households_src","land_area_m2","land_area_m2_src"]
    ]
    if not bad.empty:
        st.warning("ë¶ˆì¼ì¹˜ ìƒ˜í”Œ(ìµœëŒ€ 20ê±´):")
        st.dataframe(bad.head(20), use_container_width=True)

st.markdown("**ë‹¨ì§€ ëª©ë¡**")

df_list = df_map[[
    "apt_id",
    "address_display",
    "org_name", "biz_type", "op_type",
    "status",
    "households", "land_area_m2",
    "far",
    "floors_show",
]].copy()

df_list["households"]   = pd.to_numeric(df_list["households"], errors="coerce")
df_list["land_area_m2"] = pd.to_numeric(df_list["land_area_m2"], errors="coerce")
df_list["far"]          = pd.to_numeric(df_list["far"], errors="coerce")

# ===== í•„í„° ì˜ì—­ =====
fcol1, fcol2, fcol3, fcol4 = st.columns([1.6, 1.4, 1.6, 1.2])
with fcol1:
    kw = st.text_input("ê²€ìƒ‰ì–´(ì£¼ì†Œ/ì¡°í•©ëª…/í‚¤ì›Œë“œ)", value="", placeholder="ì˜ˆ) ê°œí¬, ëª©ë™, ì¡°í•©")

HH_BUCKETS = {
    "ì „ì²´": (None, None),
    "~ 300ì„¸ëŒ€": (0, 300),
    "301â€“500ì„¸ëŒ€": (301, 500),
    "501â€“1,000ì„¸ëŒ€": (501, 1000),
    "1,001â€“2,000ì„¸ëŒ€": (1001, 2000),
    "2,001ì„¸ëŒ€ ì´ìƒ": (2001, None),
}

AREA_BUCKETS = {
    "ì „ì²´": (None, None),
    "~ 30,000 mÂ²": (0, 30000),
    "30,001â€“50,000 mÂ²": (30001, 50000),
    "50,001â€“100,000 mÂ²": (50001, 100000),
    "100,001â€“200,000 mÂ²": (100001, 200000),
    "200,001 mÂ² ì´ìƒ": (200001, None),
}

with fcol2:
    hh_choice = st.selectbox("ì„¸ëŒ€ìˆ˜ ë²”ì£¼", list(HH_BUCKETS.keys()), index=0)
with fcol3:
    la_choice = st.selectbox("ë©´ì  ë²”ì£¼(mÂ²)", list(AREA_BUCKETS.keys()), index=0)
with fcol4:
    hide_zero = st.checkbox("0/ê²°ì¸¡ì¹˜ ìˆ¨ê¸°ê¸°", value=True)

mask = pd.Series(True, index=df_list.index)

if kw.strip():
    _kw = kw.strip().lower()
    mask &= (
        df_list["address_display"].fillna("").str.lower().str.contains(_kw) |
        df_list["org_name"].fillna("").str.lower().str.contains(_kw)
    )

hh_lo, hh_hi = HH_BUCKETS[hh_choice]
la_lo, la_hi = AREA_BUCKETS[la_choice]

col_series = df_list["households"].fillna(-1)
if hh_lo is not None:
    mask &= col_series >= hh_lo
if hh_hi is not None:
    mask &= col_series <= hh_hi

col_series = df_list["land_area_m2"].fillna(-1)
if la_lo is not None:
    mask &= col_series >= la_lo
if la_hi is not None:
    mask &= col_series <= la_hi

if hide_zero:
    mask &= df_list["households"].fillna(0) > 0
    mask &= df_list["land_area_m2"].fillna(0) > 0

filtered = df_list[mask].reset_index().rename(columns={"index": "orig_index"})

scol1, scol2 = st.columns([1.2, 1.2])
with scol1:
    sort_key = st.selectbox("ì •ë ¬ ê¸°ì¤€",
        ["ì„¸ëŒ€ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ", "ì„¸ëŒ€ìˆ˜ ì˜¤ë¦„ì°¨ìˆœ", "ë©´ì  ë‚´ë¦¼ì°¨ìˆœ", "ë©´ì  ì˜¤ë¦„ì°¨ìˆœ", "ì£¼ì†Œ ì˜¤ë¦„ì°¨ìˆœ"], index=0)
with scol2:
    topn = st.selectbox("í‘œì‹œ ê°œìˆ˜", [10, 20, 50, 100, "ì „ì²´"], index=1)

if sort_key == "ì„¸ëŒ€ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ":
    filtered = filtered.sort_values("households", ascending=False, na_position="last")
elif sort_key == "ì„¸ëŒ€ìˆ˜ ì˜¤ë¦„ì°¨ìˆœ":
    filtered = filtered.sort_values("households", ascending=True, na_position="last")
elif sort_key == "ë©´ì  ë‚´ë¦¼ì°¨ìˆœ":
    filtered = filtered.sort_values("land_area_m2", ascending=False, na_position="last")
elif sort_key == "ë©´ì  ì˜¤ë¦„ì°¨ìˆœ":
    filtered = filtered.sort_values("land_area_m2", ascending=True, na_position="last")
elif sort_key == "ì£¼ì†Œ ì˜¤ë¦„ì°¨ìˆœ":
    filtered = filtered.sort_values("address_display", ascending=True, na_position="last")

if topn != "ì „ì²´":
    filtered = filtered.head(int(topn))

show_df = filtered[[
    "orig_index",
    "address_display", "org_name", "biz_type", "op_type",
    "status",
    "households", "land_area_m2",
    "far", "floors_show",
]].copy().rename(columns={
    "address_display": "ì£¼ì†Œ",
    "org_name": "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…",
    "biz_type": "ì‚¬ì—…êµ¬ë¶„",
    "op_type": "ìš´ì˜êµ¬ë¶„",
    "status": "ì§„í–‰ë‹¨ê³„",
    "households": "ê³„íšì„¸ëŒ€ìˆ˜",
    "land_area_m2": "ë©´ì ",
    "far": "ìš©ì ë¥ (%)",
    "floors_show": "ì¸µìˆ˜",
})

show_df.insert(1, "ì„ íƒ", False)

curr_ids = show_df["orig_index"].tolist()
if (st.session_state.selected_row is None) or (st.session_state.selected_row not in curr_ids):
    st.session_state.selected_row = int(curr_ids[0]) if curr_ids else None
show_df.loc[show_df["orig_index"] == st.session_state.selected_row, "ì„ íƒ"] = True

edited = st.data_editor(
    show_df,
    use_container_width=True,
    hide_index=True,
    disabled=[
        "orig_index", "ì£¼ì†Œ", "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…", "ì‚¬ì—…êµ¬ë¶„", "ìš´ì˜êµ¬ë¶„",
        "ì§„í–‰ë‹¨ê³„", "ê³„íšì„¸ëŒ€ìˆ˜", "ë©´ì ", "ìš©ì ë¥ (%)", "ì¸µìˆ˜"
    ],
    column_config={
        "orig_index": st.column_config.NumberColumn("ì›ë³¸ì¸ë±ìŠ¤", help="ë‚´ë¶€ ì„ íƒìš©", width="small"),
        "ì„ íƒ": st.column_config.CheckboxColumn("ì„ íƒ", help="ì´ í–‰ì„ ì„ íƒ"),
        "ì£¼ì†Œ": st.column_config.TextColumn("ì£¼ì†Œ"),
        "ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…": st.column_config.TextColumn("ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…"),
        "ì‚¬ì—…êµ¬ë¶„": st.column_config.TextColumn("ì‚¬ì—…êµ¬ë¶„"),
        "ìš´ì˜êµ¬ë¶„": st.column_config.TextColumn("ìš´ì˜êµ¬ë¶„"),
        "ì§„í–‰ë‹¨ê³„": st.column_config.TextColumn("ì§„í–‰ë‹¨ê³„"),
        "ê³„íšì„¸ëŒ€ìˆ˜": st.column_config.NumberColumn("ê³„íšì„¸ëŒ€ìˆ˜", format="%,d"),
        "ë©´ì ": st.column_config.NumberColumn("ë©´ì (mÂ²)", format="%,d"),
        "ìš©ì ë¥ (%)": st.column_config.NumberColumn("ìš©ì ë¥ (%)", format=",.1f"),
        "ì¸µìˆ˜": st.column_config.TextColumn("ì¸µìˆ˜"),
    },
    key=f"project_table_{selected_gu}",
)

prev = st.session_state.selected_row
sel_list = [int(x) for x in edited.loc[edited["ì„ íƒ"] == True, "orig_index"].tolist()]

if len(sel_list) == 0:
    if prev in curr_ids:
        st.session_state.selected_row = int(prev)
    elif curr_ids:
        st.session_state.selected_row = int(curr_ids[0])
elif len(sel_list) == 1:
    st.session_state.selected_row = int(sel_list[0])
else:
    if prev in sel_list:
        new_choice = next((x for x in sel_list if x != prev), sel_list[0])
    else:
        new_choice = sel_list[0]
    st.session_state.selected_row = int(new_choice)
    st.rerun()

if st.session_state.selected_row is None:
    st.info("ì„ íƒëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()
selected_row = st.session_state.selected_row

# âœ… ì§€ë„ ë°ì´í„°/ë ˆì´ì–´ ë§Œë“¤ê¸° (selected_row í™•ì • ì´í›„)
filtered_indices = edited["orig_index"].tolist()
map_data = df_map.loc[filtered_indices].reset_index(drop=True)

# â¬‡ï¸ ì¶”ê°€
def _point_tooltip(row):
    addr = row.get("address_display", "")
    gu = row.get("gu", "")
    hh = row.get("households", "")
    la = row.get("land_area_m2", "")
    return (f"<b>{addr}</b><br/>"
            f"ìì¹˜êµ¬: {gu}<br/>"
            f"ì„¸ëŒ€ìˆ˜: {hh}<br/>"
            f"êµ¬ì—­ë©´ì (mÂ²): {la}")

map_data = map_data.copy()
map_data["tooltip_html"] = map_data.apply(_point_tooltip, axis=1)

highlight_row = df_map.loc[[selected_row]].assign(_selected=True)
highlight_row = highlight_row.copy()
highlight_row["tooltip_html"] = highlight_row.apply(_point_tooltip, axis=1)


sel_lat = float(df_map.loc[selected_row, "lat"])
sel_lon = float(df_map.loc[selected_row, "lon"])
view_state = pdk.ViewState(latitude=sel_lat, longitude=sel_lon, zoom=12.5)

layer_points = pdk.Layer(
    "ScatterplotLayer",
    data=map_data,
    get_position='[lon, lat]',
    get_radius=60,
    pickable=True,
    get_fill_color=[255, 140, 0, 160],
    get_line_color=[255, 255, 255],
    line_width_min_pixels=0.5,
)

highlight_row = df_map.loc[[selected_row]].assign(_selected=True)
layer_highlight = pdk.Layer(
    "ScatterplotLayer",
    data=highlight_row,
    get_position='[lon, lat]',
    get_radius=150,
    pickable=False,
    get_fill_color=[0, 200, 255, 220],
    get_line_color=[0, 0, 0],
    line_width_min_pixels=1.2,
)

tooltip = {
    "html": "{tooltip_html}",
    "style": {"backgroundColor": "#0f172a", "color": "white"},
}


with col12_right:
    st.markdown("### ğŸ§¾ 1ì‚¬ë¶„ë©´ Â· ê¸°ì¡´ ë‹¨ì§€ ì •ë³´")
    current = df_map.loc[selected_row]
    with st.container(border=True):
        st.markdown("**ê¸°ì¡´ ë‹¨ì§€ ì •ë³´**")
        st.markdown(
            f"- ì£¼ì†Œ: **{current['address_display']}**\n\n"
            f"- ì¶”ì§„ìœ„ì›íšŒ/ì¡°í•©ëª…: **{current['org_name']}**\n\n"
            f"- ìì¹˜êµ¬: **{current['gu']}**\n\n"
            f"- ê³„íš ì„¸ëŒ€ìˆ˜: **{int(current['households']) if pd.notna(current['households']) else 'ë¯¸ìƒ'} ì„¸ëŒ€**\n\n"
            f"- ì •ë¹„êµ¬ì—­ë©´ì : **{int(current['land_area_m2']):,} mÂ²**"
        )

# 3â€“4ì‚¬ë¶„ë©´ ë ˆì´ì•„ì›ƒ ì»¬ëŸ¼
col3, col4 = st.columns([1.6, 1.4], gap="large")

# === 3ì‚¬ë¶„ë©´: í˜¼ì¡ë„ ê·¸ë˜í”„ ===
with st.spinner("êµí†µ ê¸°ì¤€ë…„ë„ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
    if TRAFFIC_XLSX_PATH.exists():
        ensure_speed_csv(TRAFFIC_XLSX_PATH, TRAFFIC_CSV_PATH)
    elif not TRAFFIC_CSV_PATH.exists():
        st.warning(f"ê¸°ì¤€ CSVê°€ ì—†ìŠµë‹ˆë‹¤: {TRAFFIC_CSV_PATH.name}\n"
                   f"â†’ data í´ë”ì— {TRAFFIC_XLSX_PATH.name} ë¥¼ ë„£ìœ¼ë©´ ìë™ ë³€í™˜ë©ë‹ˆë‹¤.")

sel_lat = float(current.get("lat", 37.5667))
sel_lon = float(current.get("lon", 126.9784))

def _to_norm_str_id(s):
    return (
        pd.Series(s, dtype="object")
          .astype(str)
          .str.replace(r"\.0$", "", regex=True)
          .str.strip()
    )

with col3:
    st.markdown("### ğŸš¦ 3ì‚¬ë¶„ë©´ Â· ì£¼ë³€ ë„ë¡œ í˜¼ì¡ë„ (ê¸°ì¤€ë…„ë„)")

    radius = st.slider("ë°˜ê²½(m)", 500, 3000, 1000, step=250, key="radius_m")
    max_links = st.slider("í‘œì‹œ ë§í¬ ìˆ˜", 5, 20, 10, step=1, key="max_links")

    df_plot = None
    if TRAFFIC_CSV_PATH.exists() and SHP_PATH.exists():
        if _HAS_PLOT_SPEED:
            chart_or_fig, df_plot = plot_speed(
                csv_path=TRAFFIC_CSV_PATH,
                shp_path=SHP_PATH,
                center_lon=sel_lon,
                center_lat=sel_lat,
                radius_m=radius,
                max_links=max_links,
                renderer="altair",
                chart_height=700,
            )
            # ğŸ”¸ ì§€ë„ ì¦‰ì‹œ ë Œë”í•˜ë˜ PathLayer/combined_layers í˜¸ì¶œì€ ì‚­ì œ(ë Œë” ê¸ˆì§€)

            # Altair Chartì´ë©´ st.altair_chart()ë¡œ í‘œì‹œ
            if isinstance(chart_or_fig, alt.Chart):
                st.altair_chart(chart_or_fig, use_container_width=True, theme=None)
            else:
                st.pyplot(chart_or_fig, use_container_width=True)
        else:
            fig, df_plot = plot_nearby_speed_from_csv(
                csv_path=TRAFFIC_CSV_PATH,
                shp_path=SHP_PATH,
                center_lon=sel_lon,
                center_lat=sel_lat,
                radius_m=radius,
                max_links=max_links,
            )
            st.pyplot(fig, use_container_width=True)

        if df_plot is not None:
            with st.expander("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                st.dataframe(
                    df_plot.sort_values(["link_id", "hour"]).head(300),
                    use_container_width=True
                )

        # === 3ì‚¬ë¶„ë©´: ì‹œê°„ëŒ€ ê¸°ì¤€ ê²°ê³¼ â†’ SHP ë§¤ì¹­ â†’ GeoJSON(ì‹œê°„ëŒ€) ì €ì¥ ===
        try:
            if df_plot is not None and not df_plot.empty:
                shp = gpd.read_file(SHP_PATH)[[LINK_ID_COL, "geometry"]]
                if shp.crs is None or (shp.crs.to_epsg() != 4326):
                    shp = shp.to_crs(epsg=4326)

                shp["link_id_norm"] = (
                    pd.to_numeric(shp[LINK_ID_COL], errors="coerce").round().astype("Int64").astype(str)
                )
                ids = (
                    df_plot["link_id"].astype(str).str.replace(r"\.0$", "", regex=True).unique().tolist()
                )
                link_gdf = shp[shp["link_id_norm"].isin(ids)].copy()

                st.session_state["matched_links_geojson"] = link_gdf.__geo_interface__
            else:
                st.session_state["matched_links_geojson"] = None
        except Exception as e:
            st.session_state["matched_links_geojson"] = None
            st.info(f"ë§í¬ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        st.info("êµí†µ CSV ë˜ëŠ” SHPê°€ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

                # === í˜¼ì¡ë„ / í˜¼ì¡ë¹ˆë„ê°•ë„ í† ê¸€ ê·¸ë˜í”„ ===
    if 'df_plot' in locals() and df_plot is not None and not df_plot.empty:
        st.markdown("### ğŸ“ˆ í˜¼ì¡ì§€í‘œ ë¹„êµ (í˜¼ì¡ë„ vs í˜¼ì¡ë¹ˆë„ê°•ë„)")

        def compute_congestion_from_speed(df_plot):
            d = df_plot.copy()
            d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")
            d["free_flow"] = d.groupby("link_id")["í‰ê· ì†ë„(km/h)"].transform("max").clip(lower=1)
            d["í˜¼ì¡ë„(%)"] = ((1 - (d["í‰ê· ì†ë„(km/h)"] / d["free_flow"]).clip(0, 1)) * 100).clip(0, 100)
            d["ì§€í‘œëª…"] = "í˜¼ì¡ë„"
            return d[["link_id", "hour", "í˜¼ì¡ë„(%)", "ì§€í‘œëª…"]]

        def compute_congestion_freq_intensity(df_plot, boundary_speed=30):
            d = df_plot.copy()
            d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")
            d["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = (d["í‰ê· ì†ë„(km/h)"] <= boundary_speed).astype(int) * 100
            d = d.groupby(["link_id", "hour"], as_index=False)["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"].mean()
            d["ì§€í‘œëª…"] = "í˜¼ì¡ë¹ˆë„ê°•ë„"
            return d

        def compute_cfi_weighted_robust(
                speed_df: pd.DataFrame,
                vol_df: pd.DataFrame,
                boundary_mode: str = "percentile",
                boundary_value: float = 30.0,
                min_samples: int = 1
        ):
            d = speed_df.copy()
            d["link_id"] = d["link_id"].astype(str)
            d["hour"] = pd.to_numeric(d["hour"], errors="coerce").astype("Int64")
            d["í‰ê· ì†ë„(km/h)"] = pd.to_numeric(d["í‰ê· ì†ë„(km/h)"], errors="coerce")

            v = vol_df.copy()
            v["link_id"] = v["link_id"].astype(str)
            v["hour"] = pd.to_numeric(v["hour"], errors="coerce").astype("Int64") % 24
            v["ì°¨ëŸ‰ëŒ€ìˆ˜"] = pd.to_numeric(v["ì°¨ëŸ‰ëŒ€ìˆ˜"], errors="coerce").fillna(0)

            m = d.merge(v, on=["link_id", "hour"], how="inner").dropna(subset=["í‰ê· ì†ë„(km/h)"])

            if boundary_mode == "percentile":
                p = float(boundary_value)
                p = max(5.0, min(95.0, p))
                boundary = np.nanpercentile(m["í‰ê· ì†ë„(km/h)"], p)
            else:
                boundary = float(boundary_value)

            m["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] = (m["í‰ê· ì†ë„(km/h)"] <= boundary).astype(int) * m["ì°¨ëŸ‰ëŒ€ìˆ˜"]

            g = (m.groupby(["link_id", "hour"], as_index=False)
                 .agg(ì „ì²´ì°¨ëŸ‰ìˆ˜=("ì°¨ëŸ‰ëŒ€ìˆ˜", "sum"),
                      í˜¼ì¡ì°¨ëŸ‰ìˆ˜=("í˜¼ì¡ì°¨ëŸ‰ìˆ˜", "sum")))

            g.loc[g["ì „ì²´ì°¨ëŸ‰ìˆ˜"] < max(1, min_samples), ["í˜¼ì¡ì°¨ëŸ‰ìˆ˜", "ì „ì²´ì°¨ëŸ‰ìˆ˜"]] = np.nan
            g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = (g["í˜¼ì¡ì°¨ëŸ‰ìˆ˜"] / g["ì „ì²´ì°¨ëŸ‰ìˆ˜"]) * 100
            g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"] = g["í˜¼ì¡ë¹ˆë„ê°•ë„(%)"].fillna(0).clip(0, 100)

            g.attrs = {"boundary": boundary, "mode": boundary_mode}
            return g

        metric_choice = st.radio(
            "í‘œì‹œí•  í˜¼ì¡ì§€í‘œ ì„ íƒ",
            ["í˜¼ì¡ë„", "í˜¼ì¡ë¹ˆë„ê°•ë„"],
            horizontal=True,
            index=0,
            key="metric_toggle"
        )

        if metric_choice == "í˜¼ì¡ë„":
            df_metric = compute_congestion_from_speed(df_plot).rename(columns={"í˜¼ì¡ë„(%)": "value"})
            y_title = "í˜¼ì¡ë„ (0=ììœ ì£¼í–‰, 100=ë§¤ìš°í˜¼ì¡)"
        else:
            vol_path = DATA_DIR / "TrafficVolume_Seoul_2023.csv"
            if vol_path.exists():
                vol_norm = load_volume_csv(vol_path)
                bcol1, bcol2, bcol3 = st.columns([1, 1, 1])
                with bcol1:
                    boundary_mode = st.radio("ê²½ê³„ë°©ì‹", ["percentile", "fixed"], horizontal=True, index=0, key="bd_mode")
                with bcol2:
                    if boundary_mode == "percentile":
                        boundary_value = float(st.slider("ì†ë„ë¶„í¬ ë¶„ìœ„ìˆ˜(%)", 10, 90, 40, 5, key="bd_pct"))
                    else:
                        boundary_value = float(st.number_input("ê³ ì • ê²½ê³„ì†ë„(km/h)", 10.0, 100.0, 30.0, 1.0, key="bd_fix"))
                with bcol3:
                    band_kmh = float(st.slider("ì™„í™” ë°´ë“œí­ (km/h)", 5, 20, 10, 1, key="bd_band"))

                df_cfi = compute_cfi_soft(
                    df_plot, vol_norm,
                    boundary_mode=boundary_mode,
                    boundary_value=boundary_value,
                    tau_kmh=band_kmh
                )
                used_boundary = getattr(df_cfi, "attrs", {}).get("boundary", None)
                if used_boundary is not None:
                    st.caption(f"ì‚¬ìš©ëœ ê²½ê³„ì†ë„ â‰ˆ {used_boundary:.1f} km/h (ë°´ë“œí­ {band_kmh:.1f} km/h)")

                df_metric = df_cfi.rename(columns={"í˜¼ì¡ë¹ˆë„ê°•ë„(%)": "value"})
                y_title = "í˜¼ì¡ë¹ˆë„ê°•ë„ (êµí†µëŸ‰ ê°€ì¤‘ Â· Soft)"
            else:
                df_metric = compute_congestion_freq_intensity(df_plot).rename(columns={"í˜¼ì¡ë¹ˆë„ê°•ë„(%)": "value"})
                y_title = "í˜¼ì¡ë¹ˆë„ê°•ë„ (í˜¼ì¡êµ¬ê°„ ì°¨ëŸ‰ë¹„ìœ¨)"

        CHART_H = 400
        HALF_W = 1100

        chart = (
            alt.Chart(df_metric)
            .mark_line(point=True)
            .encode(
                x=alt.X("hour:Q", title="ì‹œê°„ëŒ€ (ì‹œ)"),
                y=alt.Y("value:Q", title=y_title, scale=alt.Scale(domain=[0, 100])),
                color=alt.Color(
                    "link_id:N",
                    title="ë§í¬ ID",
                    legend=alt.Legend(orient="bottom", direction="horizontal", columns=4),
                ),
                tooltip=[
                    alt.Tooltip("link_id:N", title="ë§í¬"),
                    alt.Tooltip("hour:Q", title="ì‹œ"),
                    alt.Tooltip("value:Q", title=y_title, format=".1f"),
                ],
            )
            .properties(title=f"{metric_choice} ë³€í™” ì¶”ì´", width=HALF_W, height=CHART_H)
            .configure_view(strokeWidth=0)
        )
        st.altair_chart(chart, use_container_width=False, theme=None)

        if metric_choice == "í˜¼ì¡ë„":
            st.markdown("### ğŸ§® í˜¼ì¡ë„(%) ì •ì˜")
            st.markdown("- ë§í¬ $(l)$, ì‹œê°„ëŒ€ $(h)$ì—ì„œì˜ í‰ê· ì†ë„ë¥¼ $v_{l,h}$ ë¼ í•  ë•Œ,")
            st.latex(r"v_{\mathrm{ff},l}=\max v_{l,h}")
            st.latex(r"\mathrm{í˜¼ì¡ë„}_{l,h}(\%)=\Big(1-\min\big(1,\frac{v_{l,h}}{v_{\mathrm{ff},l}}\big)\Big)\times 100")
            st.markdown("- ê°’ì˜ ì˜ë¯¸: **0% = ììœ ì£¼í–‰**, **100% = ë§¤ìš° í˜¼ì¡**")
        else:
            st.markdown("### ğŸ§® í˜¼ì¡ë¹ˆë„ê°•ë„(%) ì •ì˜ (êµí†µëŸ‰ ê°€ì¤‘ Â· Soft)")
            st.markdown("- ê²½ê³„ì†ë„ $v_b$ ë¶€ê·¼ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜ë˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í™•ë¥ ë¡œ í˜¼ì¡ ì—¬ë¶€ë¥¼ ê·¼ì‚¬í•©ë‹ˆë‹¤.")
            st.latex(r"p_{\mathrm{cong}}(v)=\frac{1}{1+\exp\!\left(\frac{v-v_b}{\tau}\right)}")
            st.markdown("- ë§í¬Â·ì‹œê°„ëŒ€ë³„ í˜¼ì¡ë¹ˆë„ê°•ë„ëŠ” **êµí†µëŸ‰ ê°€ì¤‘ í‰ê· **ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.")
            st.latex(r"""\mathrm{CFI}_{l,h}(\%)=100\times
            \frac{\sum_i w_{l,h,i}\,p_{\mathrm{cong}}(v_{l,h,i})}{\sum_i w_{l,h,i}}""")
            st.markdown("- ì—¬ê¸°ì„œ $w$ëŠ” ì°¨ëŸ‰ëŒ€ìˆ˜, $\\tau$ëŠ” ì „í™˜ì˜ ë¶€ë“œëŸ¬ì›€ì„ ì œì–´í•˜ëŠ” ë°´ë“œí­(km/h)ì…ë‹ˆë‹¤.")

        # === [MAKE DAILY GEOJSON] 3ì‚¬ë¶„ë©´ df_metric â†’ ì¼í‰ê·  GeoJSON ì €ì¥ (ë Œë” ê¸ˆì§€) ===
        def _color_from_value(v):
            if pd.isna(v): return (200, 200, 200)
            v = float(v)
            if v < 30:   return (0, 200, 0)       # ì´ˆë¡
            if v < 70:   return (255, 200, 0)     # ë…¸ë‘
            return (255, 0, 0)                    # ë¹¨ê°•

        try:
            if df_metric is not None and not df_metric.empty and SHP_PATH.exists():
                # 1) ì¼í‰ê·  ì§‘ê³„
                df_daily = (
                    df_metric.groupby("link_id", as_index=False)["value"]
                             .mean()
                             .rename(columns={"value": "daily_value"})
                )
                df_daily["link_id_norm"] = _to_norm_str_id(df_daily["link_id"])

                # 2) SHP ë§¤ì¹­
                gdf_link = gpd.read_file(SHP_PATH)[[LINK_ID_COL, "geometry"]]
                if gdf_link.crs and gdf_link.crs.to_epsg() != 4326:
                    gdf_link = gdf_link.to_crs(epsg=4326)
                gdf_link["link_id_norm"] = (
                    pd.to_numeric(gdf_link[LINK_ID_COL], errors="coerce").round().astype("Int64").astype(str)
                )
                gdf_vis = gdf_link.merge(df_daily, on="link_id_norm", how="inner")

                if not gdf_vis.empty:
                    # 3) GeoJSON propertiesì— ìƒ‰ìƒê°’ ì¶”ê°€
                    cols = list(zip(*gdf_vis["daily_value"].apply(_color_from_value)))
                    gdf_vis["color_r"], gdf_vis["color_g"], gdf_vis["color_b"] = cols[0], cols[1], cols[2]

                    # â¬‡ï¸ ì¶”ê°€: ë§í¬ ì „ìš© íˆ´íŒ HTML ìƒì„±
                    gdf_vis["tooltip_html"] = (
                            "<b>ë§í¬:</b> " + gdf_vis["link_id_norm"].astype(str) +
                            "<br/><b>ì¼í‰ê·  í˜¼ì¡ë„:</b> " + gdf_vis["daily_value"].round(1).astype(str) + "%"
                    )

                    # 4) ì„¸ì…˜ ì €ì¥ (ì¼í‰ê· )
                    st.session_state["matched_links_geojson_daily"] = gdf_vis.__geo_interface__
                else:
                    st.session_state["matched_links_geojson_daily"] = None
        except Exception as e:
            st.session_state["matched_links_geojson_daily"] = None
            st.info(f"ì¼í‰ê·  í˜¼ì¡ë„ GeoJSON ìƒì„± ì˜¤ë¥˜: {e}")




# ================================================================
# ğŸ—ºï¸ 1â€“2ì‚¬ë¶„ë©´ ë‹¨ì¼ ë Œë” ë¸”ë¡ (daily â†’ hourly â†’ points/highlight)
# ================================================================
with col12_left:
    layers = []
    # 1) ì¼í‰ê·  í˜¼ì¡ë„ GeoJSON ìš°ì„ 
    gj_daily = st.session_state.get("matched_links_geojson_daily")
    if gj_daily:
        layer_links = pdk.Layer(
            "GeoJsonLayer",
            data=gj_daily,
            pickable=True,
            auto_highlight=True,
            get_line_color='[properties.color_r, properties.color_g, properties.color_b, 220]',
            lineWidthMinPixels=3,
        )
        layers.append(layer_links)
    else:
        # 2) ì—†ìœ¼ë©´ ì‹œê°„ëŒ€ ê¸°ì¤€ GeoJSON
        gj_hourly = st.session_state.get("matched_links_geojson")
        if gj_hourly:
            layer_links = pdk.Layer(
                "GeoJsonLayer",
                data=gj_hourly,
                pickable=True,
                auto_highlight=True,
                get_line_color=[255, 80, 80, 220],
                lineWidthMinPixels=3,
            )
            layers.append(layer_links)

    # 3) ê¸°ë³¸ ì /í•˜ì´ë¼ì´íŠ¸ëŠ” í•­ìƒ ì¶”ê°€
    layers += [layer_points, layer_highlight]

    map_slot.pydeck_chart(
        pdk.Deck(
            layers=layers,
            initial_view_state=view_state,
            tooltip=tooltip,
        )
    )

# -------------------------------------------------------------
# ğŸ’¡ 4ì‚¬ë¶„ë©´ Â· ì‹ ì„¤ì¡°ê±´ ì…ë ¥ + ê¸°ëŒ€íš¨ê³¼ + ë¦¬í¬íŠ¸ (ì¤‘ë³µ key í•´ê²° ë²„ì „)
# -------------------------------------------------------------
with col4:
    st.markdown("### ğŸ§¾ 4ì‚¬ë¶„ë©´ Â· ì‹ ì„¤ì¡°ê±´ ì…ë ¥ ë° ê¸°ëŒ€íš¨ê³¼")

    st.markdown("**ğŸ—ï¸ ì¬ê±´ì¶• ë‹¨ì§€ ì¡°ê±´ ì…ë ¥**")
    with st.expander("ğŸ“‹ ì¡°ê±´ ì„¤ì •", expanded=True):
        # ì „ìš©í‰í˜• ì…ë ¥ (ê±´ì„¤ì‚¬ ë‚´ë¶€ ì¶”ì • or ê³„íšì•ˆ ê¸°ì¤€)
        desired_py = st.number_input(
            "ì „ìš©í‰í˜• (í‰)",
            min_value=10.0,
            max_value=60.0,
            value=25.0,
            step=1.0,
            key="desired_py_4q",
            help="ë‹¨ì§€ í‰ê·  ì „ìš©ë©´ì (í‰)ì„ ì…ë ¥í•©ë‹ˆë‹¤. ì˜ˆ: 25í‰í˜•, 34í‰í˜• ë“±"
        )

        # ë²„ìŠ¤ ìš©ëŸ‰ (í˜¼ì¡ë„ ì‚°ì •ìš©)
        bus_capacity = st.number_input(
            "ë²„ìŠ¤ 1ëŒ€ ì •ì› (ëª…)",
            min_value=10,
            max_value=100,
            value=45,
            step=1,
            key="bus_capacity_4q"
        )

        # ëŒ€ì¤‘êµí†µ ì¦ì„¤ ë¹„ìœ¨ (ê°€ìƒ ë³€ìˆ˜)
        bus_increase_ratio = st.slider(
            "ë²„ìŠ¤ ì¦í¸ ë¹„ìœ¨ (%)",
            0,
            100,
            20,
            step=5,
            key="bus_increase_ratio_4q"
        )

        # êµí†µ ê°œì„ ë¹„ìš© (ê°€ìƒ ë³€ìˆ˜)
        infra_invest_cost = st.number_input(
            "êµí†µ ì¸í”„ë¼ íˆ¬ìë¹„ìš© (ì–µì›)",
            min_value=0.0,
            max_value=500.0,
            value=50.0,
            step=5.0,
            key="infra_invest_cost_4q"
        )

    st.markdown("---")

    # KPI ê³„ì‚° (ê°€ìƒ ëª¨ë¸)
    st.markdown("### ğŸ“Š KPI ê²°ê³¼")

    # ê¸°ë³¸ í˜¼ì¡ë„ ì˜ˆì¸¡ (ì„¸ì…˜ì—ì„œ í‰ê·  í˜¼ì¡ë„ ê°€ì ¸ì˜´)
    base_congestion = st.session_state.get("avg_congestion", 50.0)

    # ë‹¨ìˆœ íš¨ê³¼ ëª¨ë¸ (ë²„ìŠ¤ ì¦í¸ìœ¨ë¡œ í˜¼ì¡ë„ ê°œì„ , íˆ¬ìë¹„ ëŒ€ë¹„ ìˆ˜ìµë¥  ê³„ì‚°)
    predicted_congestion = max(0, base_congestion * (1 - bus_increase_ratio / 150))
    expected_margin_billion = max(0, (100 - predicted_congestion) * (bus_increase_ratio / 10)) - infra_invest_cost

    # KPI ì¶œë ¥
    kpi1, kpi2 = st.columns(2)
    kpi1.metric("ì˜ˆìƒ í˜¼ì¡ë„(%)", f"{predicted_congestion:.1f} %", delta=f"{base_congestion - predicted_congestion:.1f} â†“")
    kpi2.metric("ì´ ê¸°ëŒ€ ìˆ˜ìµ", f"{expected_margin_billion:.1f} ì‹­ì–µ ì›")

    # ì„¸ì…˜ ë°˜ì˜
    st.session_state["predicted_congestion"] = predicted_congestion
    st.session_state["expected_margin_billion"] = expected_margin_billion
    st.session_state["desired_py"] = desired_py

    st.markdown("---")

    # PDF ë¦¬í¬íŠ¸ ì €ì¥ ê¸°ëŠ¥
    from reportlab.lib.pagesizes import A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
    from reportlab.lib.styles import getSampleStyleSheet

    def generate_pdf_report(kpi_data):
        pdf_path = "AIoT_Dashboard_Report.pdf"
        styles = getSampleStyleSheet()
        doc = SimpleDocTemplate(pdf_path, pagesize=A4)
        content = [
            Paragraph("AIoT ìŠ¤ë§ˆíŠ¸ ì¸í”„ë¼ ëŒ€ì‹œë³´ë“œ ë¦¬í¬íŠ¸", styles["Title"]),
            Spacer(1, 12),
            Paragraph(f"ì˜ˆìƒ í˜¼ì¡ë„: {kpi_data['predicted_congestion']:.1f} %", styles["Normal"]),
            Paragraph(f"ì´ ê¸°ëŒ€ ìˆ˜ìµ: {kpi_data['expected_margin_billion']:.1f} ì‹­ì–µ ì›", styles["Normal"]),
            Paragraph(f"í‰í˜•(ì „ìš©): {kpi_data['desired_py']} í‰", styles["Normal"]),
        ]
        doc.build(content)
        st.success("ğŸ“„ PDF ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! (AIoT_Dashboard_Report.pdf)")

    if st.button("ğŸ§¾ PDF ë¦¬í¬íŠ¸ ì €ì¥"):
        kpi_data = {
            "predicted_congestion": predicted_congestion,
            "expected_margin_billion": expected_margin_billion,
            "desired_py": desired_py,
        }
        generate_pdf_report(kpi_data)
